---
layout: post
category: Ceph
title: ceph安装部署
tagline: by 噜噜噜
tags: 
  - ceph
published: true
---



<!--more-->

### 一、环境

部署工具：ceph-deploy

操作系统：centos7

ceph版本：rpm-mimic 【可根据ceph源来替换】

### 二、节点准备

##### 0、配置主机名，并加到部署节点的hosts文件中

##### 1、配置yum源

`在所有节点上执行`

基础源：

```
sudo wget -O CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo  #下载阿里云的base源
sudo wget -O epel.repo http://mirrors.aliyun.com/repo/epel-7.repo #下载阿里云的epel源

sudo sed -i '/aliyuncs/d' /etc/yum.repos.d/CentOS-Base.repo
sudo sed -i 's/gpgcheck=1/gpgcheck=0/g' /etc/yum.repos.d/CentOS-Base.repo
sudo sed -i '/aliyuncs/d' /etc/yum.repos.d/epel.repo
```

ceph源：【这里可换成其他版本】

```
sudo vim /etc/yum.repos.d/ceph.repo
[ceph]
name=ceph
baseurl=http://mirrors.aliyun.com/ceph/rpm-mimic/el7/x86_64/
gpgcheck=0
priority =1
[ceph-noarch]
name=cephnoarch
baseurl=http://mirrors.aliyun.com/ceph/rpm-mimic/el7/noarch/
gpgcheck=0
priority =1
[ceph-source]
name=Ceph source packages
baseurl=http://mirrors.aliyun.com/ceph/rpm-mimic/el7/SRPMS
gpgcheck=0
priority=1
```

更新软件库：

```
sudo yum update && sudo yum clean all && sudo  yum makecache
```

##### 2、安装ntp并配置

```
sudo yum install ntp ntpdate ntp-doc
sudo systemctl start ntpd && sudo systemctl enable ntpd
```

##### 3、创建普通用户并配置免密

`你的管理节点必须能够通过 SSH 无密码地访问各 Ceph 节点，如果 ceph-deploy以某个普通用户登录，那么这个用户必须有无密码使用 sudo的权限`

①创建普通用户并设置sudo权限

```
sudo useradd -d /home/{username} -m {username}  #创建普通用户
sudo passwd {username}
echo "{username}  ALL=(ALL) NOPASSWD: ALL" >> /etc/sudoers
```

②在部署节点上创建上面创建的普通用户的密钥

```
ssh-keygen
ssh-copy-id {username}@node1     #将公钥拷贝到ceph节点
ssh-copy-id {username}@node2
ssh-copy-id {username}@node3
....
```

##### 4、安装ceph-deploy工具

`只在部署节点上安装`

```
sudo yum install ceph-deploy
```

##### 5、关闭requiretty

`找到 Defaults requiretty 选项，把它改为 Defaults:ceph !requiretty 或者直接注释掉`

```
Defaults:ceph !requiretty 
```

### 三、安装ceph

**以下操作都使用普通用户**

`集群要达到active + clean 状态，最少需要一个Monitor 和两个 OSD 守护进程`

##### 1、创建新目录

管理节点上创建一个目录，用于保存 ceph-deploy 生成的配置文件和密钥对

```
mkdir my-cluster  #在当前用户的家目录下创建，只有此目录才有权限创建
cd my-cluster
```

##### 2、创建集群

```
ceph-deploy new {initial-monitor-node(s)} #创建集群，new后面是mon节点主机名 ,如有多个空格隔开
```

![](https://s3.ax1x.com/2020/12/04/DbnqxK.png)

执行完之后，在此目录下应该有一个 Ceph 配置文件、一个 monitor 密钥环和一个日志文件

##### 3、更改ceph配置文件 

`根据个人集群配置`

```
osd pool default size = 2  #将默认副本数改成2，这样两个osd也可达到active+clean
public network = {ip-address}/{netmask}   #【写入正确的网段】
cluster network = {ip-address/netmask}
```

注：如果你有多个网卡，可以把 public network 写入 Ceph 配置文件的 [global] 段  ，如果只有一个网络，public network 是必须写的

##### 4、安装ceph

```
ceph-deploy install {ceph-node} [{ceph-node} ...] 
```

##### 5、初始mon并收集所有密钥

```
ceph-deploy mon create-initial  #复制就行
```

完成上述操作后，当前目录里应该会出现这些密钥环

- {cluster-name}.client.admin.keyring
- {cluster-name}.bootstrap-osd.keyring
- {cluster-name}.bootstrap-mds.keyring
- {cluster-name}.bootstrap-rgw.keyring
- {cluster-name}.bootstrap-mgr.keyring

`只有在安装 Hammer 或更高版时才会创建 bootstrap-rgw 密钥环`

`如果此步失败并输出类似于如下信息 “Unable to find /etc/ceph/ceph.client.admin.keyring”，请确认 ceph.conf 中为 monitor 指定的 IP 是 Public IP，而不是 Private IP`

##### 6、增加osd

```
ceph-deploy disk list {node-name [node-name]...} #列举一节点上的磁盘
```

![](https://s3.ax1x.com/2020/12/04/DbK4hR.png)

```
ceph-deploy disk zap {osd-server-name}:{disk-name}  #擦净磁盘
举例：ceph-deploy disk zap osdserver1:sdb

注：或者是ceph-deploy disk zap node1 /dev/sdb  【2.0版本的】
```

`以下模式可单独使用或者混合使用：`

**①使用filestore**

使用filestore采用journal模式（每个节点数据盘需要两块盘或两个分区） 创建逻辑卷

```
vgcreate data /dev/sdg   (SSD盘)
lvcreate --size 40G --name log data  ##创建逻辑卷

ceph-deploy osd create  --filestore   --fs-type xfs --data /dev/sdc  --journal data/log node1
...
```

**②使用bluestore** 

`下面是使用快速和慢速设备的混合，如果不是混合的，则不需要单独为block.db创建逻辑卷`

```
vgcreate cache /dev/sdb  
lvcreate --size 100G --name db-lv-0 cache  ##创建逻辑卷

ceph-deploy osd create   --bluestore  --data /dev/sdc --block-db cache/db-lv-0 --block-wal cache/wal-lv-0  node2
...
```

非混合盘：

```
ceph-deploy osd create --data /dev/vdb {node-name} 
```

**注：**wal & db 的大小问题

使用混合机械和固态硬盘设置时，block.db为Bluestore创建足够大的逻辑卷非常重要 。通常，block.db应该具有 尽可能大的逻辑卷。 建议block.db尺寸不小于4％ block。例如，如果block大小为1TB，则block.db 不应小于40GB。 如果不使用快速和慢速设备的混合，则不需要为block.db（或block.wal）创建单独的逻辑卷。Bluestore将在空间内自动管理这些内容block

##### 7、将ceph.client.admin.keyring拷贝到各个节点上

```
ceph-deploy admin {admin-node} {ceph-node}
```

##### 8、安装MGR

```
ceph-deploy mgr create  {ceph-node}...
```

##### 9、验证集群状态

```
ceph -s ##在安装了ceph客户端的节点上执行
```

### 四、安装ceph其他服务【可选】

##### 0、添加MON和MGR节点

```
ceph-deploy mon add {ceph-nodes}
ceph-deploy mgr create {ceph-nodes}
```

##### 1、安装MDS服务

`如果要使用cephFS，至少需要一个元数据服务器`

```
ceph-deploy --overwrite-conf mds create {ceph-nodes}
```

##### 2、安装RGW网关节点

①修改端口【可选】

RGW默认使用Civetweb作为其Web Sevice，而Civetweb默认使用端口7480提供服务，如果想修改端口（如8080端口），就需要修改Ceph的配置文件。在配置文件中增加一个section[client.rgw.xx] ##xx是RGW节点主机名

```
[client.rgw.xx]
rgw_frontends = "civetweb port=8080"
```

![](https://s3.ax1x.com/2020/12/04/DbN6wq.png)

②推送配置文件到所有节点

```
ceph-deploy --overwrite-conf config push <节点名1> <节点名2> ...
```

③部署ceph object gateway

```
ceph-deploy install –-rgw {ceph-nodes}
```

④给新节点推送ceph.client.admin.keyring。如果是旧节点就不需要执行

```
ceph-deploy admin {ceph-nodes}
```

⑤安装RGW实例

```
ceph-deploy rgw create {ceph-nodes}
```

![](https://s3.ax1x.com/2020/12/04/DbwUBj.png)

一旦RGW开始运行，我们就可以通过端口7480（如果没有修改的话）来访问，我们已经改成了8080端口。如：

![](https://s3.ax1x.com/2020/12/04/Dbw44x.png)



##### 3、创建S3、Swift用户来访问RGW

①创建S3用户

想正常的访问RGW，需要创建相应的RGW用户，并赋予相应的权限，radosgw-admin命令实现了这些功能

执行下面命令，来创建一个名为testuser的用户：

```
radosgw-admin user create --uid="testuser" --display-name="First User"
```

![](https://s3.ax1x.com/2020/12/04/DbB70H.png)

**注意**：需要记住返回结果中keys->access_key和keys->secret_key的值，用于S3接口访问确认

**测试S3接口：**

需要创建一个Python测试脚本来测试S3访问。该脚本会连接RGW，创建一个bucket并列出所有的bucket。其中，变量access_key和secret_access的值，来自于创建S3用户命令时，radosgw-admin命令返回的keys->access_key和keys->secret_key。
　　执行以下步骤，首先安装python-boto库，该库用于连接S3：

```
$ apt-get install python-boto
```

　　创建并编辑Python脚本：

```
$ vi s3test.py

import boto.s3.connection

access_key = 'I0PJDPCIYZ665MW88W9R'
secret_key = 'dxaXZ8U90SXydYzyS5ivamEP20hkLSUViiaR+ZDA'
conn = boto.connect_s3(
        aws_access_key_id=access_key,
        aws_secret_access_key=secret_key,
        host='{hostname}', port={port},
        is_secure=False, calling_format=boto.s3.connection.OrdinaryCallingFormat(),
       )

bucket = conn.create_bucket('my-new-bucket')
for bucket in conn.get_all_buckets():
    print "{name} {created}".format(
        name=bucket.name,
        created=bucket.creation_date,
    )
```

　　替换{hostname}、{port}为真实环境的hostname（或者ip）和RGW的端口。执行命令进行测试：

```
$ python s3test.py
```

　　输出应该类似如下：

```
my-new-bucket-2 2017-02-19T04:34:17.530Z
```



②创建swift用户

Swift用户是作为subuser的子用户被创建的，执行以下命令：

```
radosgw-admin subuser create --uid=testuser --subuser=testuser:swift --access=full
```

![](https://s3.ax1x.com/2020/12/04/DbBxc8.png)

**新建 secret key:**

```
radosgw-admin key create --subuser=testuser:swift --key-type=swift --gen-secret
```

![](https://s3.ax1x.com/2020/12/04/DbD9BQ.png)

swift_secret_key：2FwJZB5yBn3EwnhR6dSpA6JyuryxjZsxc5AGBkiA

**测试 Swift接口** 

在任何局域网机器上安装swift环境：

```
yum install python-setuptools
yum install  python-pip
 pip install --upgrade setuptools
 pip install --upgrade python-swiftclient
```

执行下面的命令验证 swift 访问:

```
swift -A http://{IP ADDRESS}:{port}/auth/1.0 -U testuser:swift -K '{swift_secret_key}' list
```



swift -A http://172.16.22.190:8080/auth/1.0  -U testuser:swift -K '2FwJZB5yBn3EwnhR6dSpA6JyuryxjZsxc5AGBkiA' list

\##没有输出则表示可以访问