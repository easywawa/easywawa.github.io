---
layout: post
category: Linux
title: 系统性能调优实施细则
tagline: by 噜噜噜
tags: 
  - 调优
published: true
---



<!--more-->

### 一、从安装linux系统开始调优

#### 1、系统基础配置与调优

##### ①系统安装和分区优化

raid选择、linux分区和swap

磁盘分区默认安装会使用LVM进行分区管理，但是作为线上环境，不推荐使用LVM:

- ​	LVM的动态扩容功能，对于大硬盘时代来说无意义
- ​	会影响磁盘读写性能，不便于运维。一旦故障，数据基本无法恢复

在大内存的现在，swap空间还有必要吗？答案是需要：

- ​	对于内存不够大的机器，设置swap可以再内存不够用的时候不至于出发oom-killer
- ​	有些业务，如redis、elasticsearch主要使用物理内存的系统，不希望它使用swap,可以设置/proc/sys/vm/swappiness的比例

##### ②ssh登录系统策略

提高SSH连接速度：

- ​	UseDNS no   ##不适用DNS反查
- ​	GSSAPIAuthentication no   ##关闭GSSAPI验证

`UseDNS 选项打开状态下，当通过终端登录SSH服务器时，服务器端先根据客户端的IP地址进行DNS PTR反向查询出客户端的主机名，然后根据查询出的客户端主机名进行DNS正向A记录查询，验证与其原始IP地址是否一致，这是防止客户端欺骗的一种措施，但一般我们的是动态IP不会有PTR记录，打开这个选项没什么用，不如关闭`

`GSSAPIAuthentication是基于 GSSAPI 的用户认证，服务器端默认启用了GSSAPI。登陆的时候客户端需要对服务器端的IP地址进行反解析，如果服务器的IP地址没有配置PTR记录，那么就会在这里卡住`



##### ③重要文件加锁

chattr 必须通过root用户来执行

+i : 不可变的

+a : 只能添加数据，不能删除

##### 

#### 2、系统安全和防护机制

TCP_Wrappers 防火墙的实现是通过/etc/hosts.allow 和/etc/hosts.deny来实现的

```
service:hosts(s) [:action]
```

### 二、Linux 内核参数调优

- **/proc/sys/net 是跟网络相关的内核参数**
- **/proc/sys/kernel 是跟内核相关的内核参数**
- **/proc/sys/vm 是跟内存相关的内核参数**
- **/proc/sys/fs 是跟文件系统相关的内核参数**

更改方法：

`临时（重启失效）`

​	echo "xx" > /proc/sys/kernel/xx

​	sysctl -w kernel.xx="xxx" ##推荐用这种，它会检查数据的一致性

`永久`

​	将配置写道/etc/sysctl.conf文件中，然后执行sysctl -p

#### 1、网络内核参数优化

##### ①常用在**web服务器**的优化项：

**net.ipv4.tcp_syn_retries**  **[建议设置为2]**

​	nyiet.ipv4.tcp_syn_retries 的设置,表示应用程序进行connect()系统调用时，在对方不返回SYN + ACK的情况下(也就是超时的情况下)，第一次发送之后，内核最多重试几次发送SYN包;并且决定了等待时间



##### ②keepalive相关的几个参数

`TCP keepalive是指TCP连接建立后会通过keepalive的方式一直保持，不会在数据传输完成后立刻中断，而是通过keepalive机制检测连接状态。Linux控制keepalive有三个参数：保活时间net.ipv4.tcp_keepalive_time、保活时间间隔net.ipv4.tcp_keepalive_intvl、保活探测次数net.ipv4.tcp_keepalive_probes，默认值分别是 7200 秒（2 小时）、75 秒和 9 次探测`

对于一个已经建立的tcp连接。如果在keepalive_time时间内双方没有任何的数据包传输，则开启keepalive功能的一端将发送keepalive数据包，若没有收到应答，则每隔keepalive_intvl时间再发送该数据包，发送keepalive_probes次。一直没有收到应答，则发送rst包关闭连接。若收到应答，则将计时器清零

**net.ipv4.tcp_keepalive_time** **[默认是7200s 建议修改为300s]**

**net.ipv4.tcp_keepalive_intvl** **[默认75s 建议30s]**

**net.ipv4.tcp_keepalive_probes** **[默认为9次 建议5次]**



##### ③net.ipv4.tcp_orphan_retries **[默认是0]**

表示孤儿Socket废弃前重试的次数



##### ④SYN攻击相关的三个参数

`针对SYN攻击，启用SYN Cookies、设置SYN队列最大长度以及设置SYN+ACK 最大重试次数：`

**net.ipv4.tcp_syncookies** [默认是1 已开启]

表示开启SYN Cookies。

**net.ipv4.tcp_max_syn_backlog** [加大长度为8192]   **##这个是半连接队列长度**

表示SYN队列最大长度。这个参数是使用内存资源换取更大的等待队列长度，让攻击数据包不至于沾满所有连接。

当处于**SYN_RECV状态**的TCP连接数超过tcp_max_syn_backlog后，会丢弃后续的SYN报文

**net.ipv4.tcp_synack_retries** **[默认5次，建议少点]**

此参数决定内核在放弃连接之前所发送的SYN+ACK的数目



##### ⑤ time_wait状态sockets相关的几个参数：

**net.ipv4.tcp_tw_recycle [默认关闭0，更改会导致系统出现各种问题]**

表示开启TCP连接中time_wait 状态sockets的快速回收

当开启了tcp_tw_recycle选项后，当连接进入TIME_WAIT状态后，会记录对应远端主机最后到达分节的时间戳。如果同样的主机有新的分节到达，且时间戳小于之前记录的时间戳，即视为无效，相应的数据包会被丢弃

[引发的系统问题]: https://blog.csdn.net/zhuyiquan/article/details/68925707

**net.ipv4.tcp_tw_reuse [默认关闭]**

表示开启重用TIME_WAIT状态的TCP连接。启用的时候，必须同时开启快速回收recycle(tcp_tw_recycle为1)

**net.ipv4.tcp_fin_timeout**

表示处于TIME_WAIT专题的连接在回收前必须等待的最小时间



##### ⑥backlog的值：

backlog的值是网络连接过程中，某种状态的队列的长度；如果并发过高，那么会导致backlog的队列占满，服务器就会丢掉传进来的其他连接，然后就会出现客户端连接失败的情形

**net.core.netdev_max_backlog**[默认1000 建议3000]

表示网卡设备的backlog, 因为网卡接收数据包的速度远大于内核处理这些数据包的速度，允许送到队列的数据包的最大数目



**net.core.somaxconn** [默认为128 建议4096]   **##全连接队列长度**

此参数用来设置Socket监听（listen）的backlog上限。当Server处理请求较慢，以至于监听队列被填满之后，新来的请求会被拒绝



注：每一个处于监听(Listen)状态的端口,都有自己的监听队列.监听队列的长度

```
其实，对于 Nginx/Tomcat 等这种 Web 服务器，都提供了 backlog 参数设置入口，
当然它们都会有默认值，通常这个默认值都不会太大（包括内核默认的半连接队列和全连接队列长度）。
如果应用并发访问非常高，只增大应用层 backlog 是没有意义的，因为可能内核参数关于连接队列设置的都很小，
一定要综合应用层 backlog 和内核参数一起看，通过公式很容易调整出正确的设置
```

##### ⑦和滑动窗口相关的几个参数 【暂未搞清楚】

**net.core.rmem_max** : 表示接收套接字缓冲区大小的最大值

**net.core.wmem_max** : 表示发送套接字缓冲区大小的最大值

**net.ipv4.tcp_rmem** ：用来配置读缓冲的大小，第一个值是最小值，第二值为默认值，第三个值为最大值

**net.ipv4.tcp_wmem**：用来配置写缓冲的大小，第一个值是最小值，第二值为默认值，第三个值为最大值



#### 2、系统kernel参数优化

##### ①kernel.panic  [默认为0]

用来设置如果发生kernel panic，则内核在重新引导之前等待的时间（以s为单位）默认不重启

##### ②kernel.pid_max [默认为32768]

该值用来设置Linux下进程数量的最大值，一般默认值是够用的

##### ③kernel.ctrl-alt-del [默认为0]

这个值表示系统在接收到（Ctrl+Alt+Delete）组合键时的反应：0为重启 1表示不捕获



#### 3、内存内核参数优化

##### ①几个脏页有关的参数

 **vm.dirty_background_ratio** [默认为10]

 此参数指定了当文件系统缓存脏数据达到系统内存**百分之多少**时，就会触发后台回写进程将一定缓存的脏页异步的刷入磁盘

**vm.dirty_ratio**[默认为30]

 此参数表示当缓存脏数据达到内存**百分之多少**时，新的I/O请求将会被阻止，直到脏数据被写进磁盘。

`在磁盘写入不是很频繁的场景，适当的增大此值可以极大的提高文件系统的写性能。但是如果是持续、恒定的写入场合，应该降低其数值`

`dirty_background_ratio是软限制。dirty_ratio是硬限制，必须大于或等于软限制`



**vm.dirty_expire_centisecs** [默认为3000 表示30s]

 此参数表示脏数据在内存中驻留的时间，如果超过此值则下一次会被pdflush进程写入到磁盘。

**vm.dirty_writeback_centisecs** [默认是500，表示5s]

 此参数表示pdflush进程的刷新时间间隔

`如果系统是持续的写入动作，那么建议降低这个数值，可以把尖峰的写操作削平成多次写操作。相反如果是短期的尖峰式写操作，并且写入数据不大且内存又比较富裕，应该增加此数值`



##### ②vm.min_free_kbytes 

 此值表示Linux 最低保留的空闲内存，内核根据内存大小自动计算，保持默认即可

##### ③vm.overcommit_memory [默认为0]

 此参数指定了对内存分配的策略

```
0 为缺省值，它允许overcommit，但过于明目张胆的overcommit会被拒绝，比如malloc一次性申请的内存大小就超过了系统总内存。Heuristic的意思是“试探式的”，内核利用某种算法（对该算法的详细解释请看文末）猜测你的内存申请是否合理，它认为不合理就会拒绝overcommit

1 – Always overcommit. 允许overcommit，对内存申请来者不拒

2 – Don’t overcommit. 禁止overcommit
```

##### ④vm.panic_on_oom [默认为0]

 此参数表示内存不够时候内核是否直接panic, 0表示触发oom-killer 杀掉最耗内存的进程。1表示会同会panic

```
kernel维护者一份oom_socre数据，它包含各个进程的oom_score,可以在/proc/${pid}/oom_score中查看各个进程的值（值越大越容易被杀掉），这个值事根据oom_adj 运算后的结果

/proc/${pid}/oom_adj 表示被杀掉的权重，范围-17~15，当等于-17时候永远都不会被杀掉
```



##### ⑤vm.swappiness[默认30]

 此参数表示使用swap分区的概率，等于0 的时候最大限度的使用物理内存，等于100的时候表示积极的使用swap分区

#### 4、文件系统内核参数优化

##### ①fs.file-max

**此参数制定了可以分配的文件句柄的最大数目**



### 三、内存资源（物理内存/虚拟内存）性能调优

#### 1、Cache和Buffer

Cache: 就是为了弥补高速设备和低速设备之间的矛盾而建立的一个中间层。如下图：（c接口的速率接近A，d接口的速率至少等于B）

过程：A第一次从B取文件，还是会慢，而cd区域从B取到文件后，一方面传送给A，一方面自己保留一份。下次A再来取B的数据，cd久直接给A一个自己保留下来的备份，而c接口的速率接近A，这样A就不会等待多余时间了。

![](https://img02.sogoucdn.com/app/a/100520146/9f7706c753b4fdc50bc9e84e1074da2a)

Buffer:他存在目的适用于速度快的设备向速度慢的设备输出数据，例如内存的数据要写到磁盘，cpu寄存器中的数据写到内存。

**buffer是根据磁盘的读写进行设计的，它把分散的写操作集中进行，减少磁盘碎片和硬盘的反复寻道，而从提高系统性能**

过程：还是以上图为例，A要向B发送一个文件，对于A来说直接向cd区域传输数据从而交给B设备，而c接口的速率和A差不多，这样A把数据交给cd区域之后就做其他事情了，不需要等待。而对于cd区域来说，它不立即交给B数据，而是先缓存下来，除非B执行sync命令。由于d接口的速率要大于B设备，这样B也不会存在等待时间。



`结论`

①缓存Cache 是把读取过的数据保存起来，重启读取的时候若命中就不要去读磁盘了。若没有命中，则再读磁盘，其中的数据会根据读取频率进行组织，把最频繁读取的内存放在最容易找到的位置，把不再读的内容不断往后排，直到删除

②Buffer是即将要被写入磁盘的，而Cache是被从磁盘中读取出来的

③Linux中有一个守护进程定期清空缓冲Buffer内容,也可以通过sync命令手动清空缓冲 【kdmflush】进程

#### 2、种类

①Buffer Cache（块缓冲）

**块缓冲，通常1K，对应于一个磁盘块，用于减少磁盘IO**

**由物理内存分配，通常空闲内存全是bufferCache，当其他程序需要更多内存时，系统会减少cahce大小。**

***\*设立buffer cache的目的是为在程序多次访问同一磁盘块时，减少访问时间\****

应用层面，不直接与BufferCache交互，而是与PageCache交互

读文件：

   直接从bufferCache中读取

写文件：

​    方法一，写bufferCache，后写磁盘

​    方法二，写bufferCache，后台程序合并写磁盘



**BufferCache是磁盘级别的缓存/缓冲**

②Page Cache（页面缓存）

页缓冲/文件缓冲，通常4K，***\*由若干个磁盘块组成（物理上不一定连续），也即由若干个bufferCache组成\****

读文件：

   可能不连续的几个磁盘块--->bufferCache--->pageCache--->应用程序进程空间

写文件：

​    pageCache--->bufferCache--->磁盘



文件读取是由外存上不连续的几个磁盘块，到buffer cache，然后组成page cache，然后供给应用程序。

**PageCache 是文件系统层级的缓存**

③Buffer page（缓冲页）

如果内核需要单独访问一个块，就会涉及到buffer page，并会检查对应的buffer head

④Swap cache（交换缓存）

swapcached，它表示交换缓存的大小。***\*Page cache是磁盘数据在内存中的缓存，而swap cache则是交换分区在内存中的临时缓存\****

#### 3、PageCache的优化及回收

①Page Cache的优化

两个系统参数：

**vm.dirty_background_ratio**：这个参数指定了当文件系统缓存脏页（PageCache中的数据称为脏页数据）数量达到系统内存百分之多少时（默认10%）,会触发pdflush/flush/kdmflush 等后台回写进程运行，将一定缓存的脏页异步地刷入磁盘

**vm.dirty_ratio**：这个参数指定了当文件系统缓存脏页数量达到系统内存百分之多少时候（默认20%），系统不得不开始处理缓存脏页

`作为通用优化设置，建议将vm.dirty_background_ration设置成5%，vm.dirty_ratio设置成10%`

②Cache的回收

Linux 内核有自动释放Cache的机制，但是有时急需要内存资源或，**可以手动释放，但是执行前必须要执行sync**

```
释放PageCache:   echo 1 > /proc/sys/vm/drop_caches

释放文件节点（inodes）缓存和目录项缓存（dentries）: echo 2 > /proc/sys/vm/drop_caches

释放Page Cache 、文件节点（inodes）缓存和目录项缓存（dentries）: echo 3 > /proc/sys/vm/drop_caches
```

#### 4、Swap的使用和优化

①使用

`创建交换空间必须使用dd命令来完成，同时这个文件必须位于本地硬盘上，不能在网络文件系统中`

```
dd if=/dev/zero of=/data/swapfile bs=1024 count=65536
```

bs=bytes表示同时设置读写块的大小，此参数可代替ibs、obs

`要使用Swap, 首先要激活Swap，通过mkswap命令指定该作为交换空间的设备或文件`

```
mkswap /data/swapfile
```

`最后，通过swapon 命令激活swap`

```
swapon /data/swapfile
```

②优化

swappiness的值的大小对如何使用swap分区有很大的联系：

```
0：表示最大限度的使用物理内存

100：表示积极的使用swap分区，并且把内存上的数据及时的搬运到swap空间

60（Linux系统的默认值）：表示系统的物理内存使用到100-60=40%的时候，就可以使用交换分区
```

### 四、磁盘I/O与文件系统方面的性能调优

#### 1、磁盘I/O性能调优

##### ①内核块设备**IO调度队列长度**优化

可以适当的增加队列长度，可以提升磁盘的吞吐量，当然也会导致延迟

```
cat /sys/block/sdx/queue/nr_requests
```

##### ②请求在**磁盘设备上的队列深度**

尝试修改过，但是报错... 不知道为何

```
cat /sys/block/sda/device/queue_depth
```



[nr_requests 和 queue_depth的区别]: https://my.oschina.net/u/4255930/blog/4888801

##### ③选择合适的IO调度算法

`IO的调度算法（电梯算法）：`

- as(Anticipatory) : 预料I/O调度

  本质上与Deadline一样,但在最后一次读操作后,要等待6ms,才能继续进行对其它I/O请求进行调度

  它会在每个6ms中插入新的I/O操作,而会将一些小写入流合并成一个大写入流,用写入延时换取最大的写入吞吐量

  AS适合于写入较多的环境,比如文件服务器

- cfq(Complete Fairness Queueing) : 完全公平排队

  CFQ试图均匀地分布对I/O带宽的访问,避免进程被饿死并实现较低的延迟,是deadline和as调度器的**折中**

  CFQ对于多媒体应用(video,audio)和桌面系统是最好的选择
  `CFQ为每个进程/线程,单独创建一个队列来管理该进程所产生的请求,也就是说每个进程一个队列,各队列之间的调度使用时间片来调度,
  以此来保证每个进程都能被很好的分配到I/O带宽.I/O调度器每次执行一个进程的4次请求`

- deadline : 最终期限

  Deadline确保了在一个截止时间内服务请求,这个截止时间是可调整的,而默认读期限短于写期限.这样就防止了写操作因为不能被读取而饿死的现象

  **提供了最小的读取延迟和尚佳的吞吐，特别适合读取较多的环境，如数据库、web服务**

  Deadline对数据库环境(ORACLE RAC,MYSQL等)是最好的选择

- noop(No Operation) ：电梯式调度

  NOOP倾向饿死读而利于写

  **适用于SSD,有RAID卡做了磁盘阵列的环境**

总结：

- cfq是一种比较通用的调度算法，是一种以进程为出发点考虑的调度算法，保证尽量公平
- deadline是一种以**提高机械硬盘吞吐量**为思考出发点的调度算法，只有当有IO请求达到最终期限的时候才进行调度，非常适合业务比较单一并且IO压力比较重的业务，比如数据库
- noop在固态硬盘这种场景下，使用noop是最好的，deadline次之，而cfq由于复杂度的原因，效率最低



**I/O调度方法的查看与设置**

```
cat /sys/block/sda/queue/scheduler  ##查看当前磁盘的I/O调度方法
[noop] deadline cfq
```

```
dmesg | grep -i scheduler  ##查看该系统下IO支持的调度
[    0.702496] io scheduler noop registered
[    0.705111] io scheduler deadline registered (default)
[    0.708509] io scheduler cfq registered
```

```
echo cfq > /sys/block/sda/queue/scheduler  ##临时更改I/O调度方法更改到cfq
```

```
grubby --update-kernel=ALL --args="elevator=deadline" ##永久更改I/O调度方法，修改内核引导参数，加入elevator=调度程序名
reboot
```

[调度算法测试方式]: https://blog.csdn.net/cod42960/article/details/100232874?utm_medium=distribute.pc_relevant_t0.none-task-blog-searchFromBaidu-1.control&amp;depth_1-utm_source=distribute.pc_relevant_t0.none-task-blog-searchFromBaidu-1.control

但是我测试的时候确没什么大的差距，使用dd的时候注意不要使用缓存`iflag=direct`

##### ④预读扇区数优化

预读是提高磁盘性能的有效手段，目前对**顺序读**比较有效，主要利用数据的局部性特点

```
cat /sys/block/sda/queue/read_ahead_kb
128
```

或者使用:

```
blockdev --setra 512 /dev/sda     ##注意这里设置的是扇区，实际是读256字节
cat /sys/block/sda/queue/read_ahead_kb
256
```

`设置大些对读大文件有用，可以有效的减少读请求的次数，但是对于随机读没有用。在SSD上甚至有害`

##### ⑤SSD优化

**TRIM**

#### 2、文件系统性能调优

##### ①xfs文件系统格式化时参数优化

```
mkfs.xfs -d agcount=16 -l size=128m,lazy-count=1,version=2 /dev/sdc
```

-d agcount=16：可以设置成1、2、4、16等，这个参数可以调节对cpu的占用率，值越小，占用率越低

-l size=128m ：默认是10m,修改这个参数可以显著的提高XFS文件系统删除文件的速度【验证过没问题】，还有复制文件的速率【验证了没啥大的差别】

lazy-count：该值可以是0或者1，如果为1则不会修改超级块，可以显著提高性能

##### ②磁盘挂载项

- noatime：禁止记录访问时间戳，提高文件系统读写性能
- nodirtime：禁止记录访问文件目录时间戳
- data=writeback： 不记录data journal，提高文件系统写入性能  【谨慎使用】
- barrier=0：barrier保证journal先于data刷到磁盘，上面关闭了journal，这里的barrier也就没必要开启了 【谨慎使用】
- nobh：关闭buffer_head，防止内核打断大块数据的IO操作
- async：异步I/O方式，提高写性能

```
mount -o noatime,data=writeback,barrier=0,nobh /dev/sda /es_data
```

##### ③EXT4文件系统参数优化

Ext4文件系统分两部分存储：

- ​	文件的元数据块
- ​	数据块

metadata和data的操作日志journal也是分开管理的，**可以让ext4记录metadata的日志，而不记录data的日志**

ext4支持三种日志模式：

- ​	journal :提供了完全的日志，所有的数据都会被先写入到日志中，再写入磁盘。在文件系统崩溃的时候，通过日志来恢复

- ​	ordered ：ext4的默认模式，ext4文件系统只提供metadata的日志。这种模块的性能比journal快

- ​	writeback ：当元数据提交到日志后，数据块可以直接被提交到磁盘，但是不保证数据和元数据的写入顺序，这种模式是ext4提供的性能最好的模式

  ```
  mount -t ext4 -o data=writeback /dev/sdb3 /mnt
  ```

  

