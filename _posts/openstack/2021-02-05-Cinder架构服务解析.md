---
layout: post
category: openstack
title: Cinder架构服务解析
tagline: by 噜噜噜
tags: 
  - openstack
published: true
---



<!--more-->

### 一、Cinder概述

#### 1、作用

正如 Nova 本身不提供 Hypervisor 技术一般，Cinder 自身也不提供存储技术，而是作为一个抽象的中间管理层，北向提供稳定而统一的 Block Storage 资源模型、南向通过 Plug-ins&Drivers 模型对接多样化的后端存储设备。所以 Cinder 的精华从不在于存储技术，而是在于对 Block Storage as a Service 需求（创建、删除、快照、挂载、分离、备份卷）的抽象与理解

#### 2、软件架构



![å¨è¿éæå¥å¾çæè¿°](http://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWctYmxvZy5jc2RuaW1nLmNuLzIwMTkwMjI2MTQ1MjQ1MTc4LnBuZz94LW9zcy1wcm9jZXNzPWltYWdlL3dhdGVybWFyayx0eXBlX1ptRnVaM3BvWlc1bmFHVnBkR2ssc2hhZG93XzEwLHRleHRfYUhSMGNITTZMeTlpYkc5bkxtTnpaRzR1Ym1WMEwwcHRhV3hyLHNpemVfMTYsY29sb3JfRkZGRkZGLHRfNzA?x-oss-process=image/format,png)

#### 3、调度机制

与 nova-scheduler 一般，cinder-scheduler 同样需要维护调度对象（存储节点）“实时” 状态，cinder-volume service 会**定期的向 cinder-scheduler service 上报存储节点状态**（注：这实际上是通过后端存储设备的驱动程序上报了该设备的状态）

- 首先判断存储节点状态，只有状态为 up 的存储节点才会被考虑。
- 创建 Volume 时，根据 Filter 和 Weight 算法选出最优存储节点。
- 迁移 Volume 时，根据 Filter 和 Weight 算法来判断目的存储节点是否符合要求

**支持的Filters：**

- AffinityFilter
- **AvailabilityZoneFilter**：可以在 cinder-volume（存储节点）的 cinder.conf 中设置 `storage_availability_zone=az1` 来指定该存储节点的 Zone。配合 AvailabilityZoneFilter，用户创建 Volume 时选择期望的 AZ，就可以实现将 Volume 创建到指定的 AZ 中了。 默认 Zone 为 nova。
- **CapabilitiesFilter**：不同的 Volume Provider 自然具有不同的特点，用户可以通过设置 Volume Type 的 extra specs 来描述这些特性，该 Filter 就是为了通过这些特性来过滤存储节点
- **CapacityFilter**：根据存储节点上的剩余空间（free_capacity_gb）大小来进行过滤，存储节点的 free_capacity_gb 正是由 cinder-volume service 上报的
  - 通过 Volume Type 的 Extra Specs 定义 Capabilities。Extra Specs 是用 Key-Value 的形式定义
  - cinder-volume 服务会在自己的配置文件 /etc/cinder/cinder.conf 中设置“volume_backend_name”这个参数，其作用是为存储节点的 Volume Provider 命名。这样，CapabilitiesFilter 就可以通过 Volume Type 的“volume_backend_name”参数筛选出指定的 Volume Provider
- DriverFilter
- IgnoreAttemptedHostsFilter
- InstanceLocalityFilter
- JsonFilter

cinder.conf 指定你需要使用的 Filters 列表，e.g.

```
[DEFAULT]
...
scheduler_driver =cinder.scheduler.filter_scheduler.FilterScheduler
scheduler_default_filters =AvailabilityZoneFilter,CapacityFilter,CapabilitiesFilter
scheduler_default_weighers=CapacityWeigher
```

**scheduler_default_filters不设置的话，cinder-scheduler默认会使用AvailabilityZoneFilter,CapacityFilter,CapabilitiesFilter 这三个filter**

经过以上Filter的过滤，cinder-scheduler会得到符合条件的host列表，然后进入weighting环节，根据weighting算法选出最优的host



**支持的Weighting:**

- AllocatedCapacityWeigher：有最小已使用空间的host胜出。 可设置allocated_capacity_weight_multiplier为正值来反转，其默认值为-1。
- CapacityWeigher：有最大可使用空间的host胜出。可设置capacity_weight_multiplier为负值来反转算法，其默认值为1
- ChanceWeigher：随机从过滤出的host中选择一个host



**cinder.conf中，scheduler_default_weighers不设置的话，cinder-scheduler默认使用 CapacityWeigher。**

### 二、cinder-volume

该服务运行在存储节点上，管理存储空间，**处理cinder数据库的维护状态的读写请求**，通过消息队列和直接在块存储设备或软件上与其他进程交互

#### 1、volume创建失败重试机制

可以在cinder.conf中使用scheduler_max_attempts来配置volume创建失败时候的重试次数，默认次数为3，值为1则表示不使用重试机制

如果volume创建失败，cinder-volume会通过RPC重新调用cinder-scheduler去创建volume，**cinder-scheduler会检查当前的重试次数是不是超过最大可重试次数**。如果没超过，它会选择下一个可以使用的host去重新创建volume。如果在规定的重试次数内仍然无法创建volume，那么会报No valid host was found错误

#### 2、从image创建volume

volume-driver首先尝试去调用driver的clone_image方法，若driver的clone-image方法不成功，则执行Cinder的默认方法

①创建一个raw的volume，设置其状态为downloading

②将image下载并拷贝到该volume

③拷贝image的metadata到volume的metadata

#### 3、从snapshot创建volume

①获取snapshot

②Cinder不提供默认实现方式，它调用各driver的create_volume_from_snapshot方法创建volume

③如果snapshot是bootable的话，需要拷贝它的metadata到新的volume上

#### 4、从volume创建volume

①获取源volume

②Cinder不提供默认实现方式，它需要调用各driver的create_cloned_volume方法来创建volume

③如果源volume是bootable的话，需要拷贝它的metadata到新的volume上

#### 5、创建原始volume

cinder-volume会调用各driver创建一个原始的volume

### 三、cinder的更多操作

#### 1、cinder extend 

①扩大 volume 的容量。为了保护现有数据，cinder 不允许缩小 volume

②状态为 Available 的 volume 才能够被 extend

③extend 操作由后端存储driver实现

#### 2、cinder migrate

两种情况：

①volume没有attach到虚机

- 如果是同一个存储上不同backend之间的迁移，需要存储的driver会直接支持存储上的migrate
- 如果是不同存储上的backend之间的volume迁移，或者存储cinder driver不支持同一个存储上backend之间的迁移，那么将使用cinder默认的迁移操作：Cinder首先创建一个新的volume，然后从源volume拷贝数据到新volume，然后将老的volume删除

②volume已经attach到虚机上

Cinder创建一个新的volume，调用Nova去将数据从源volume拷贝到新volume，然后将老的volume删除。目前只支持Compute libvirt driver

`注意：在多个backend的情况下，host必须使用host全名。比如： cinder migrate vol-b21-1 block2@lvmdriver-b22`

#### 3、cinder QOS支持

Cinder 支持 front-end 端和 back-end 端设置 QoS

-  front-end 表示 hypervisor 端，即在宿主机上设置虚拟机的 QoS，通常使用 cgroup 或者 qemu-iothrottling；
-  back-end 端指在存储设备上设置 QoS，该功能需要存储设备的支持 （**Ceph RBD mimic开始支持 QoS**）

下面是前端Qos的配置案例：

①设置Qos

```
cinder qos-create ceph-ssd-qos consumer=front-end read_bytes_sec=50000000 write_bytes_sec=50000000 read_iops_sec=400 write_iops_sec=400
```

total_bytes_sec: the total allowed bandwidth for the guest per second 

read_bytes_sec: sequential read limitation 

write_bytes_sec: sequential write limitation 

total_iops_sec: the total allowed IOPS for the guest per second 

read_iops_sec: random read limitation 

write_iops_sec: random write limitation

②创建存储类型

```
cinder type-create ceph-storage
```

③绑定存储类型和后端存储

注意：后端存储类型名称查看

```
cat /etc/cinder/cinder.conf | grep volume_backend_name | grep -v ^#
volume_backend_name = ceph
```

```
cinder --os-username admin --os-tenant-name admin type-key ceph-storage set volume_backend_name=ceph
```

④将卷类型和qos绑定

```
格式： cinder qos-associate QOS_ID   TYPE_ID

cinder qos-associate xx  xxx
```



**NOTE**：Cinder 暂时是不支持动态 QoS 的，所以即便我们修改了 QoS Policy 的数值也不能马上被应用到 Libvirt 虚拟机。但可以直接通过 virsh 指令来设定 Libvirt 虚拟机的 QoS 属性：

```
virsh blkdeviotune instance-00000029 vdb --total-bytes-sec 1024000
```

Qos 一般来说是在一个建议值上下浮动的，对于严格要求的 QoS 还可以通过指定 `--total-iops-sec-max` 来固定上限

### 四、Qos算法

[]: https://mp.weixin.qq.com/s/rqVQcF6ucgQDdrImjsSYdA
[]: https://is-cloud.blog.csdn.net/article/details/105917290

