---
layout: post
category: openstack
title: SMP和NUMA架构解析
tagline: by 噜噜噜
tags: 
  - openstack
published: true
---



<!--more-->

原文：https://blog.csdn.net/jmilk/article/details/80816827

### 一、计算平台体系结构

#### 1、SMP

##### ①概念介绍

SMP（Sysmmetric Multi-Processor，对称多处理器）,所谓对称，即处理器之间是水平的镜像关系，无有主从之分。SMP 的出现使一台计算机不再由单个 CPU 组成

##### ②特点：

- 多个处理器**共享一个集中式存储器**，且每个处理器访问存储器的时间片相同，使得工作负载能够均匀的分配到所有可用处理器上，极大地提高了整个系统的数据处理能力
- SMP 也会要求多处理器保证共享存储器的数据一致性。如果多个处理器同时请求访问共享资源，**就需要由软件或硬件实现的加锁机制来解决资源竞态的问题**

![è¿éåå¾çæè¿°](https://img-blog.csdn.net/20180626141509604?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0ptaWxr/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

##### ③不足

上述的架构设计注定没法拥有良好的处理器数量扩展性，因为**共享存储的资源竞态总是存在的**，处理器利用率最好的情况只能停留在 **2 到 4 颗**。综合来看，SMP 架构广泛的适用于 PC 和移动设备领域，能显著提升并行计算性能。但 SMP 却不适合超大规模的服务器端场景，例如：云计算

![è¿éåå¾çæè¿°](https://img-blog.csdn.net/20180626142529383?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0ptaWxr/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)



SMP 低存储带宽的问题：

由于每个CPU必须通过相同的内存总线访问相同的内存资源，因此随着CPU数量的增加，内存访问冲突将迅速增加，最终会造成CPU资源的浪费，使 CPU性能的有效性大大降低





#### 2、NUMA

##### ①概念介绍

Non-Uniform Memory Access，非一致性存储器访问，将处理器和存储器划分到不同的节点（NUMA Node），它们都拥有几乎相等的资源

![è¿éåå¾çæè¿°](https://img-blog.csdn.net/20180626143520476?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0ptaWxr/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

##### ②特点

- 在 NUMA 节点**内部会通过自己的存储总线访问内部的本地内存**，而所有 **NUMA 节点都可以通过主板上的共享总线来访问其他节点的远程内存**
- 处理器访问本地内存和远程内存的时耗并不一致，NUMA 非一致性存储器访问由此得名。而且因为节点划分并没有实现真正意义上的存储隔离，所以 NUMA 同样只会保存一份操作系统和数据库系统的副本

![è¿éåå¾çæè¿°](https://img-blog.csdn.net/2018062615162988?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0ptaWxr/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

##### ③不足

- 使用外部共享总线时可能会触发 NUMA 节点间的 Cache 同步异常，这会严重影响内存密集型工作负载的性能。当 I/O 性能至关重要时，共享总线上的 Cache 资源浪费，会让连接到远程 PCIe 总线上的设备（不同 NUMA 节点间通信）作业性能急剧下降
- 虽然 NUMA 相比于 SMP 具有更好的处理器扩展性，但因为 NUMA 没有实现彻底的主存隔离。所以 NUMA 远没有达到无限扩展的水平，最多可支持几百个 CPU

#### 3、MPP

##### ①概念介绍

Massive Parallel Processing，大规模并行处理

主要是解决 NUMA 扩展性的限制是没有完全实现资源（e.g. 存储器、互联模块）的隔离性，那么 MPP 的解决思路就是**为处理器提供彻底的独立资源**

MPP 拥有多个真正意义上的独立的 SMP 单元，每个 SMP 单元独占并只会访问自己本地的内存、I/O 等资源，SMP 单元间通过节点互联网络进行连接（Data Redistribution，数据重分配），是一个完全无共享（Share Nothing）的 CPU 计算平台结构

![](https://img-blog.csdn.net/20180627191346519?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0ptaWxr/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)![](https://img-blog.csdn.net/20180626152005762?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0ptaWxr/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

##### ②特点

- 多 SMP 单元组成，单元之间完全无共享
  每个SMP单元内都可以包含一个独立的操作系统
- MPP需要一种复杂的机制来调度和平衡各个节点的负载和并行处理过程
- MPP 架构的局部区域内存的访存延迟低于远地内存访存延迟，因此 Linux 会自定采用局部节点分配策略，当一个任务请求分配内存时，首先在处理器自身节点内寻找空闲页，如果没有则到相邻的节点寻找空闲页，如果还没有再到远地节点中寻找空闲页，在操作系统层面就实现了访存性能优化

### 二、NUMA详解

#### 1、基本对象概念

- **Node**：包含有若干个 Socket 的组

- **Socket**：表示一颗物理 CPU 的封装，简称插槽。Intel 为了避免将物理处理器和逻辑处理器混淆，所以将物理处理器统称为插槽

- **Core**：Socket 内含有的物理核

- **Thread**：在具有 Intel 超线程技术的处理器上，每个 Core 可以被虚拟为若干个（通常为两个）逻辑处理器，逻辑处理器会共享大多数内核资源（e.g. 内存缓存、功能单元）。逻辑处理器被统称为 Thread

- **Processor**：处理器的统称，可以区分为物理处理器（physical processor）和逻辑处理器（virtual processors）

  *（一个 Socket 4 个 Core)*

  ![](https://img-blog.csdn.net/20180721104344602?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0ptaWxr/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

  **包含关系**：`NUMA Node > Socket > Core > Thread`

  ![](https://img-blog.csdn.net/20180626152205520?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0ptaWxr/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

  上图为一个 NUMA Topology，表示该服务器具有 2 个 numa node，每个 node 含有一个 socket，每个 socket 含有 6 个 core，每个 core 又被超线程为 2 个 thread，所以服务器总共的 processor = 2x1x6x2 = 24 颗

  

#### 2、numa调度策略

Linux 的每个进程或线程都会延续父进程的 NUMA 策略，优先会将其约束在一个 NUMA node 内。当然了，如果 NUMA 策略允许的话，进程也可以调用其他 node 上的资源

**NUMA 的 CPU 分配策略有下列两种：**

- cpunodebind：约束进程运行在指定的若干个 node 内
- physcpubind：约束进程运行在指定的若干个物理 CPU 上

**NUMA 的 Memory 分配策略有下列 4 种：**

- localalloc：约束进程只能请求访问本地内存
- preferred：宽松地为进程指定一个优先 node，如果优先 node 上没有足够的内存资源，那么进程允许尝试运行在别的 node 内
- membind：规定进程只能从指定的若干个 node 上请求访问内存，并不严格规定只能访问本地内存
- interleave：规定进程可以使用 RR 算法轮转地从指定的若干个 node 上请求访问内存

#### 3、numa的优缺点

优点：

- 高存储访问带宽
- 有效的 Cache 效率
- 灵活 PCIe I/O 设备的布局

缺点：

- 跨节点远程内存访问不仅延时高、带宽低、消耗大，还可能需要处理数据一致性的问题



### 三、NOVA上的numa实现

#### 1、nova中定义的numa的对象

- **Cell**：NUMA Node 的通名词，供 Libvirt API 使用
- **vCPU**：虚拟机的 CPU，根据虚拟机 NUMA 拓扑的不同，一个虚拟机 CPU 可以是一个 socket、core 或 thread。
- **pCPU**：宿主机的 CPU，根据宿主机 NUMA 拓扑的不同，一个物理机 CPU 同样可以是一个 socket、core 或 thread。
- **Siblings Thread**：兄弟线程，即由同一个 Core 超线程出来的 Threads。
- **Host NUMA Topology**：宿主机的 NUMA 拓扑。
- **Guest NUMA Topology**：虚拟机的 NUMA 拓扑。

`注：Thread siblings 对象的引入是为了无论服务器是否开启了超线程，Nova 同样能够支持物理 CPU 绑定的功能`

#### 2、虚拟机的使用最优策略

​	**标准的策略是尽量将一个虚拟机完全局限在单个 NUMA 节点内**

#### 3、Guest NUMA Topology

这种技术是为了解决guest中的OS可以更好的进行进程调度和内存分配问题，将虚拟机的 Guest NUMA Node 与宿主机的 Host NUMA Node 进行关联映射。这样可以映射大块的虚拟机内存到宿主机内存，和设置 vCPU 与 pCPU 的映射

Guest NUMA Topology 实际上是将一个大资源需求的虚拟机划分为多个小资源需求的虚拟机，将多个 Guest NUMA Node 分别绑定到不同的 Host NUMA Node

`注意：如果宿主机开启了超线程，则要求将超线程特性暴露给虚拟机，并在 NUMA Node 内绑定 vCPU 与 pCPU 的关系。否则 vCPU 会被分配给 Siblings Thread，由于超线程竞争，性能远不如将 vCPU 分配到 Socket 或 Core 的好`

#### 4、在nova上的实现

**首先判断该物理服务器是否支持 NUMA 功能**：

```
dmesg | grep -i numa
[    0.000000] NUMA: Initialized distance table, cnt=2
[    0.000000] NUMA: Node 0 [mem 0x00000000-0x7fffffff] + [mem 0x100000000-0x603fffffff] -> [mem 0x00000000-0x603fffffff]
[    0.000000] Enabling automatic NUMA balancing. Configure with numa_balancing= or the kernel.numa_balancing sysctl
[    3.580288] pci_bus 0000:00: on NUMA node 0
[    3.587892] pci_bus 0000:17: on NUMA node 0
[    3.592896] pci_bus 0000:3a: on NUMA node 0
[    3.599917] pci_bus 0000:5d: on NUMA node 0
[    3.601488] pci_bus 0000:80: on NUMA node 1
[    3.606831] pci_bus 0000:85: on NUMA node 1
[    3.609111] pci_bus 0000:ae: on NUMA node 1
[    3.610622] pci_bus 0000:d7: on NUMA node 1
```

[![y91V29.png](https://s3.ax1x.com/2021/01/28/y91V29.png)](https://imgchr.com/i/y91V29)

如果输出上述内容则表示支持 NUMA，如果输出 *No NUMA configuration found* 则表示不支持

**查看物理服务器的 NUMA 拓扑**：

```
yum install -y numactl

numactl --hardware
available: 2 nodes (0-1)
node 0 cpus: 0 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40 42 44 46 48 50 52 54
node 0 size: 391816 MB
node 0 free: 179439 MB
node 1 cpus: 1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35 37 39 41 43 45 47 49 51 53 55
node 1 size: 393216 MB
node 1 free: 234424 MB
node distances:
node   0   1
  0:  10  21
  1:  21  10
```

**nova-scheduler 启用 NUMATopologyFilter**：

```
scheduler_default_filters=...,NUMATopologyFilter
```

**Nova 的 NUMA 亲和原则是**：将 Guest vCPU/Mem 都分配在同一个 NUMA Node 上，充分使用 NUMA node local memory，避免跨 Node 访问 remote memory

```
openstack flavor set FLAVOR-NAME \
    --property hw:numa_nodes=FLAVOR-NODES \
    --property hw:numa_cpus.N=FLAVOR-CORES \
    --property hw:numa_mem.N=FLAVOR-MEMORY
```

- **FLAVOR-NODES（整数）**：设定 Guest NUMA nodes 的个数。如果不指定，则 Guest vCPUs 可以在任意可用的 Host NUMA nodes 上浮动
- **N**：整数，Guest NUMA nodes ID，取值范围在 [0, FLAVOR-NODES-1]
  - N 仅仅是 Guest NUMA node 的索引，并非实际上的 Host NUMA node 的 ID。例如，Guest NUMA node 0，可能会被映射到 Host NUMA node 1。类似的，FLAVOR-CORES 的值也仅仅是 vCPU 的索引。因此，Nova 的 NUMA 特性并不能用来约束 Guest vCPU/Mem 绑定到指定的 Host NUMA node 上。要完成 vCPU 绑定到指定的 pCPU，需要借助 CPU Pinning policy 机制
- **FLAVOR-CORES（逗号分隔的整数）**：设定分配到 Guest NUMA node N 上运行的 vCPUs 列表。如果不指定，vCPUs 在 Guest NUMA nodes 之间平均分配
- **FLAVOR-MEMORY（整数）**：单位 MB，设定分配到 Guest NUMA node N 上 Memory Size。如果不指定，Memory 在 Guest NUMA nodes 之间平均分配



**设定 Guest NUMA Topology 的两种方式**：

- **自动设定 Guest NUMA Topology**
  - 仅仅需要指定 Guest NUMA nodes 的个数，然后由 Nova 根据 Flavor 设定的虚拟机规格平均将 vCPU/Mem 分布到不同的 Host NUMA nodes 上（默认从 Host NUMA node 0 开始分配）
  - 建议一同使用 `hw:numa_mempolicy` 属性，表示 NUMA 的 Mem 访问策略，有严格访问本地内存的 strict 和宽松的 preferred 两种选择
- **手动设定 Guest NUMA Topology**
  - 不仅指定 Guest NUMA nodes 的个数，还需要通过 `hw:numa_cpus.N` 和 `hw:numa_mem.N` 来指定每个 Guest NUMA nodes 上分配的 vCPUs 和 Memory Size。



**Nova Scheduler 会根据参数 `hw:numa_nodes` 来决定如何映射 Guest NUMA node**

- 如果 numa_nodes = 1，Scheduler 将会选择出单个 NUMA 节点能够满足虚拟机 flavor 配置的计算节点。
- 如果 numa_nodes > 1，Scheduler 将会选择出 NUMA 节点数量以及 NUMA 节点中资源情况能够满足虚拟机 flavor 配置的计算节点



例如：

```
openstack flavor set aze-FLAVOR \ 
  --property hw:numa_nodes=2 \ 
  --property hw:numa_cpus.0=0 \ 
  --property hw:numa_cpus.1=1,2,3 \ 
  --property hw:numa_mem.0=1024 \ 
  --property hw:numa_mem.1=3072 
```

使用该 flavor 创建的虚拟机时，最后由 Libvirt Driver 完成将 Guest NUMA node 映射到 Host NUMA node 上

还可以通过 Image Metadata 来设定：

```
glance image-update --property \ 
    hw_numa_nodes=2 \ 
    hw_numa_cpus.0=0 \ 
    hw_numa_mem.0=1024 \ 
    hw_numa_cpus.1=1,2,3 \ 
    hw_numa_mem.1=3072 \ 
    image_name
```

`注意，当镜像的 NUMA 约束与 Flavor 的 NUMA 约束冲突时，以 Flavor 为准`



`补充： CPU Binding的方式`

```
openstack flavor set 2C2G20D-NUMA-CPU_binding \
> --property hw:cpu_policy=dedicated \
> --property hw:cpu_thread_policy=isolate
```



### 四、nova上实现vCPU绑定

#### 1、vcpu_pin_set 配置项

vcpu_pin_set 是 compute node 上的 nova.conf 配置项，是 OpenStack 最早设计用于限定 Guest 可以使用 compute node 上的 pCPUs 的范围，解决了下述问题：

```
目前实例可以使用计算节点的所有pcpu，当实例的vcpu繁忙时，主机可能会变慢，所以我们需要将vcpu绑定到主机的特定pcpu上，而不是所有pcpu。libvirt的vcpu xml配置文件会变成这样：<vcpu cpuset="4-12,^8,15">1</vcpu>
```

NOTE：如果指定的 CPU 范围超出了宿主机原有的 CPU 范围则会触发异常



#### 2、CPU绑定策略

CPU 绑定策略机制经常会结合 NUMA topology 一起使用

```
openstack flavor set FLAVOR-NAME \    
--property hw:cpu_policy=CPU-POLICY \    
--property hw:cpu_thread_policy=CPU-THREAD-POLICY
```

- CPU-POLICY：
  - shared (default)：不独占 pCPU 策略，允许 vCPUs 在不同的 pCPUs 间浮动，尽管 vCPUs 受到 NUMA node 的限制也是如此。
  - dedicated：**独占 pCPU 策略**，Guest 的 vCPUs 将会严格的 pinned 到 pCPUs 的集合中。在没有明确配置 Guest NUMA Topology 的情况下，Nova 会将每个 vCPU 都作为一个 Socket 中的一个 Core；如果已经明确的配置了 Guest NUMA Topology 的话，那么虚拟机就会严格按照 Guest NUMA Topology 和 Host NUMA Topology 的映射关系将 vCPUs pinned to pCPUs，pCPUs 可能是一个 Core 或是一个 Thread，根据 Host 实际的处理器体系结构以及 CPU-THREAD-POLICY 来共同决定。此时的 **CPU overcommit ratio 为 1.0（不支持 CPU 超配），避免 vCPU 的数量大于 Core 的数量导致的线程上下文切换损耗**。
- CPU-THREAD-POLICY：
  - prefer (default)：如果 Host 开启了超线程，则 vCPU 优先选择在 Siblings Thread 中运行，即所有的 vCPU 都只会考虑 siblings。例如：4 个逻辑核属于同一 NUMA，其中 CPU1 和 CPU2 属于相同物理核，CPU3 和 CPU4 属于不同的物理核，若此时创建一个 Flavor vCPU 为 4 的云主机会创建失败，因为 siblings 只有 [set([1, 2])]；否则，vCPU 优先选择在 Core 上运行。
  - **isolate（vCPU 性能最好）**：vCPU 必须绑定到 Core 上。如果 Host 没有开启超线程，则理所当然会将 vCPU 绑定到 Core 上；相反，如果 Host 开启了超线程，则 vCPU 会被绑定到 Siblings Thread 的一个 Thread 中，并且其他的 vCPU 不会再被分配到该 Core 上，**相当于 vCPU 独占一个 Core，避免 Siblings Thread 竞争**。
  - require（vCPU 数量最多）：vCPU 必须绑定到 Thread 上。**Host 必须开启超线程，每个 vCPU 都会被绑定到 Thread 上，直到 Thread 用完为止**。如果没有开启的话，那么该 Host 不会出现在 Nova Scheduler 的调度名单中

**NOTE 1**：只有设定 `hw:cpu_policy=dedicated` 时，`hw:cpu_thread_policy` 才会生效。后者设定的是 vCPU pinning to pCPU 的策略

NOTE 2：应该使用 Host Aggregate 来区分开 pinned 和 unpinned 的虚拟机，退一步来说，最起码也应该让两者运行在不同的 NUMA 节点上。而且如果一个 compute node 上运行的全是 pinned 虚拟机，那么这个 compute node 不建议配置超配比

NOTE 3：如果 `cpu_thread_policy=prefer | require` 时，Nova 的 Thread 分配策略是尽量先占满一个 Core，然后再使用下一个 Core 上的 Thread，尽量避免 Thread/Core 碎片影响到后面创建的虚拟机



### 五、创建虚拟机报错

### TS1：Requested instance NUMA topology cannot fit the given host NUMA topology.

如果你创建虚拟机失败，并且在 nova-conductor service 的日志看见 *Requested instance NUMA topology cannot fit the given host NUMA topology*，那么你或许应该检查一下 nova-scheduler service 是否启用了 NUMATopologyFilter 以及可以检查一下是否还具有足够的 NUMA 资源了。

### TS2：Filter NUMATopologyFilter returned 0 hosts

如果你创建虚拟机是被，并且在 nova-scheduler service 的日志看见 *Filter NUMATopologyFilter returned 0 hosts*，则表示 ComputeNodes 已经没有足够的 NUMA 资源了。或者你可以考虑使用 `hw:cpu_policy=shared` 不独占 CPU 的策略。