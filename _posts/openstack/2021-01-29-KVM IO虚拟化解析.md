---
layout: post
category: openstack
title: KVM 的 I/O 虚拟化
tagline: by 噜噜噜
tags: 
  - xx
published: true
---



<!--more-->

### 一、I/O虚拟化的解决方案种类

#### 1、完全虚拟化解决方案

![xx](https://img-blog.csdnimg.cn/20200503091837398.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0ptaWxr,size_16,color_FFFFFF,t_70)

步骤：

①Guest（客户机）的设备驱动程序发起 I/O 请求操作请求

②KVM 内核模块中的 I/O 操作捕获代码拦截这次 I/O 请求

③KVM 经过加工处理后将本次 I/O 请求的信息放到 I/O 共享页（Sharing Page），并通知用户空间的 QEMU 程序

④QEMU 程序获得 I/O 操作的具体信息之后，交由**硬件模拟代码来模拟出本次 I/O 操作**

⑤I/O 操作完成之后，QEMU 将结果放回 I/O 共享页，并通知 KVM 内核模块中的 I/O 操作捕获代码

⑥KVM 模块的捕获代码读取 I/O 共享页中的操作结果，并把结果放回 Guest



**优点：**

- 不用修改 GuestOS，使用原生的设备驱动。
- 可以模拟一些老式经典设备，解决因为手头没有足够设备而引入的调试开发问题。

**缺点：**

-  I/O 路径长，依赖 KVM 和 QEMU 来做中间的信息处理。
- 多次进行数据拷贝。
-  Guest 和 Host 的内核态和用户态多次进行上下文切换（Context Switch）。

可见完全虚拟化方案最大的问题就是性能低下，VirtIO 就是通过实现半虚拟化的思路来解决了这一问题

#### 2、半虚拟化解决方案

在半虚拟化 I/O 模式中，GuestOS 和 Hypervisor 共同合作来完成模拟的模拟，是整个流程更加高效。但相对的，Guest 需要知道自己是一台虚拟机，所以就需要对 GuestOS进行修改（安装非原生驱动程序）

![xx](https://img-blog.csdnimg.cn/20200503092849800.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0ptaWxr,size_16,color_FFFFFF,t_70)

VirtIO 使得 GuestOS 能够实现一组通用的接口，来完成前端（GuestOS VirtIO 内核驱动）和后端（运行在 Hypervisor 中的设备模拟程序）之间的通信交互，并且后端程序并不需要是通用的，因为它们只实现前端所需的行为。通过这种解耦的方式使得 GuestOS 具备了高性能 I/O 的同时也兼具了一定的跨平台特性



**优点：**

- 标准化：VirtIO 实现了统一的设备接口，VirtIO 以及 virtio-ring 完成标准传输层队列的接口，GuestOS 内核协议栈上层可以对接各种类型设备的，如：blk、net、pci、scsi 等。
- 环形队列批量处理 I/O 请求。
- 优化内核态与用户态频繁切换、Guest 和 Host 陷入陷出带来的性能开销

**缺点：**

- Guest 需要安装 VirtIO 前端驱动程序



**总结：**

`传统的通信路径为：`

GuestOS IO-----> KVM模块拦截加工处理------>IO 共享页------->QEMU中的模拟程序----->硬件模拟代码

   最终硬件代码模拟出本次IO请求。逐层返回IO请求

`VirtIO的通信路径为：`

GuestOS IO----->GuestOS VirtIO 内核驱动------->QEMU中的模拟程序----->硬件模拟代码

   GuestOS中的驱动与qemu模拟程序之间的通信是将IO请求传递到Vring中，具体查看原理



**virtio：虚拟机中的半虚拟化前端驱动和主机上的后端服务简单的使用virtqueue共享队列交换数据**





### 二、VirtIO

#### 1、架构

![x](https://img-blog.csdnimg.cn/20200503100301963.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0ptaWxr,size_16,color_FFFFFF,t_70)

VirtIO 的架构可以分为四层：

-  驱动层: virtio-net front-end（Driver）：运行在 Guest 中的各种 I/O 设备的驱动程序模块。例如：virtio-net、virtio-blk
- 控制平面通信层: virtio（e.g. virtio-pci）：用于 Host 与 Guest 之间进行协商交换，以建立/关闭数据平面。常见的协议有：PCI、PCIe、vhost、vhost-user。本质是虚拟队列接口，作为前、后端通信的桥梁（数据结构、notify 等通信机制）。例如：virtio-net 使用两个虚拟队列（接受、发送），virtio-blk 使用一个虚拟队列
- 数据平面通信层: Transport（virtio-ring）：用于 Host 与 Guest 之间进行数据交换。实现了两个环形缓冲区，实现了具体的通信机制和数据流交换
- 设备层: virtio backend（Device）：运行在后端 Hypervisor（e.g. QEMU）处理程序模块，或直接就是一个硬件设备

NOTE：通信层之所以分开控制平面和数据平面是因为两者的侧重不同，**控制平面追求尽可能的灵活以兼容不用的设备和厂商**，而**数据平面则追求由更高的转发效率以快速的交换数据包**

#### 2、实现原理

VirtIO 利用了 Guest 可以与 Host **共享内存**的特性，作为 I/O 半虚拟化实现的底层支撑

**实现方式**：

​	Hypervisor 通过多种方式将 devices 暴露给 Guest，对于 Guest 而言，这个 devices 就像是物理设备一样

`Guest 使用 VirtIO devices 最典型的方式是通过 PCI/PCIe 协议，举例：`

**在物理环境中：**

​     PCI/PCIe 硬件设备会使用特定的物理内存地址范围，设备的驱动程序可以通过访问该内存范围来读取或写入设备的寄存器,也可以通过特殊的处理器指令来暴露其配置空间（Configuration Space）

**在虚拟环境中：**

假设GuestOS Kernel 已经加载了 VirtIO 驱动，当 Guest 启动时 Linux 操作系统会自动完成 PCI/PCIe 设备的枚举，在此过程中，VirtIO Driver（virtio-pci）会使用 PCI Vendor ID 和 PCI Device ID 来标识一个 PCI/PCIe 设备（此处我理解为映射），Kernel 再通过这些标识来知道使用什么驱动程序来处理这些设备

**数据平面：使用共享内存的数据通信部分，在控制平面去设定它们**

#### 3、VirtIO的前后端通知机制

[]: https://is-cloud.blog.csdn.net/article/details/105899307

#### 4、VirtIO的网络实现

##### ①virtio-net 驱动和设备



##### ②vhost-net 

QEMU 实现的 virtio-net device 带来的网络性能并不如意，究其原因还是因为频繁的上下文切换，低效的数据拷贝、线程间同步等问题。于是，社区在 Linux Kernel 为 virtio-net device 后端实现了一个**新的数据面**，名为 vhost-net

**vhost技术对virtio-net进行了优化，在内核中加入了vhost-net.ko模块，使得对网络数据可以在内核态得到处理**

vhost 协议允许 Hypervisor 将 VirtIO 的数据面（virtio-ring）offload到另一个组件(vhost-net)上，从而**更有效地执行数据转发**。这个组件正是 vhost-net

##### ③vhost-user

vhost-user 就是结合了 DPDK 各方面优化技术而得到的用户态 virtio-net device

DPDK ：用于加速数据中心的网络数据平面

vhost 协议通过对 vhost-net 字符设备进行 ioctl 实现，而 vhost-user 协议则通过 UNIX Socket 进行实现

