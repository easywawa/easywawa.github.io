---
layout: post
category: kubernetes
title: 快速使用kubeadm部署k8s集群
tagline: by 噜噜噜
tags: 
  - k8s
published: true
---



<!--more-->

### 一、节点环境信息

| 主机名      | IP            | 角色    | 组件                                                         |
| ----------- | ------------- | ------- | ------------------------------------------------------------ |
| k8s-master1 | 192.168.1.25  | master1 | kube-apiserver<br />kube-controller-manager<br />kube-proxy<br />etcd<br />coredns<br />kube-flanel |
| k8s-master2 | 192.168.1.136 | master2 |                                                              |
| k8s-master3 | 192.168.1.37  | master3 |                                                              |
| k8s-node1   | 192.168.1.26  | node1   | kube-proxy<br />kube-flannel                                 |
| k8s-node2   | 192.168.1.125 | node2   |                                                              |
| k8s-node3   | 192.168.1.152 | node3   |                                                              |

**k8s高可用集群：**

​	高可用体现在apiserver 的高可用处。kube-scheduler 和 kube-controller-manager 可以以集群模式运行，通过 leader 选举产生一个工作进程，其它进程处于阻塞模式。**kube-apiserver可以运行多个实例，但对其它组件需要提供统一的访问地址，该地址需要高可用**

本次部署版本：

- Kubernetes版本: 1.18.0
- 系统版本: CentOS7

### 二、部署

#### 1、前提准备

```
1. 保证所有节点接入互联网并配置好YUM源
mkdir -p /etc/yum.repos.d/bak
mv /etc/yum.repos.d/*.repo  /etc/yum.repos.d/bak
curl -o /etc/yum.repos.d/CentOS-Base.repo https://mirrors.aliyun.com/repo/Centos-7.repo
curl -o /etc/yum.repos.d/docker-ce.repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo

cat <<EOF > /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/
enabled=1
gpgcheck=1
repo_gpgcheck=1
gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
EOF


sed -i -e '/mirrors.cloud.aliyuncs.com/d' -e '/mirrors.aliyuncs.com/d' /etc/yum.repos.d/CentOS-Base.repo
yum clean all && yum makecache
2. 关闭防火墙，selinux
systemctl disable --now firewalld
systemctl disable --now NetworkManager
setenforce 0
3. 设置好主机名,并将添加到每台主机的hosts文件中
hostnamectl set-hostname xxx
比如：
#################HA K8s Cluster###############
192.168.1.25    master1
192.168.1.136   master2
192.168.1.37    master3
192.168.1.26    node1
192.168.1.125   node2
192.168.1.152   node3

4. 配置好时间同步
ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime
echo 'Asia/Shanghai' >/etc/timezone
ntpdate time2.aliyun.com
# 加入到crontab
*/5 * * * * ntpdate time2.aliyun.com
# 加入到开机自动同步，/etc/rc.local
ntpdate time2.aliyun.com

5. 关闭swap
swapoff -a
sed -i 's/.*swap/#&/' /etc/fstab

6. 所有节点配置limit
ulimit -SHn 65535

7. 安装软件
yum install ipvsadm ipset sysstat conntrack libseccomp -y

8. 配置内核参数

cat <<EOF > /etc/sysctl.d/k8s.conf
net.ipv4.ip_forward = 1
fs.may_detach_mounts = 1
vm.overcommit_memory = 1
vm.panic_on_oom = 0
fs.inotify.max_user_watches = 89100
fs.file-max = 52706963
fs.nr_open = 52706963
net.ipv4.tcp_keepalive_time = 600
net.ipv4.tcp_keepalive_probes = 3
net.ipv4.tcp_keepalive_intvl = 15
net.ipv4.tcp_max_tw_buckets = 36000
net.ipv4.tcp_tw_reuse = 1
net.ipv4.tcp_max_orphans = 327680
net.ipv4.tcp_orphan_retries = 3
net.ipv4.tcp_syncookies = 1
net.ipv4.tcp_max_syn_backlog = 16384
net.ipv4.tcp_max_syn_backlog = 16384
net.ipv4.tcp_timestamps = 0
net.core.somaxconn = 16384

EOF


sysctl -p /etc/sysctl.d/k8s.conf      

9. 加载ipvs模块
modprobe -- ip_vs
modprobe -- ip_vs_rr
modprobe -- ip_vs_wrr
modprobe -- ip_vs_sh
modprobe -- nf_conntrack_ipv4
cat << EOF>/etc/modules-load.d/ipvs.conf
ip_vs
ip_vs_rr
ip_vs_wrr
ip_vs_sh
nf_conntrack_ipv4
ip_tables
ip_set
xt_set
ipt_set
ipt_rpfilter
ipt_REJECT
ipip
EOF

systemctl enable --now systemd-modules-load.service

10. 升级内核（版本较新的不需要此步）
yum -y update kernel 
reboot

11. 检查内核
lsmod | grep --color=auto -e ip_vs -e nf_conntrack
```

#### 2、部署过程

```
1. 安装配置docker           # 所有节点
2. 安装k8s软件                 # 所有节点
3. 安装负载均衡及高可用     # 所有 Master节点
4. 初台化Master1            # Master1节点    
5. 配置kubectl              # 所有需要的节点
6. 部署网络插件             # Master1节点
7. 加入Master节点           # 其它 Master节点
8. 加入Worker节点           # 所有 Node节点
9. Metrics部署			 
10. Dashboard部署
```

##### 2.1 安装配置docker

```
yum install docker-ce -y
mkdir /etc/docker
##新版kubelet建议使用systemd，所以可以把docker的CgroupDriver改成systemd
##配置镜像加速
cat << EOF>/etc/docker/daemon.json 
{
  "exec-opts": ["native.cgroupdriver=systemd"],
  "log-driver": "json-file",
  "log-opts": {
    "max-size": "100m"
  },
  "registry-mirrors": ["https://pf5f57i3.mirror.aliyuncs.com"]
}
EOF
systemctl daemon-reload && systemctl restart docker && systemctl enable docker
```

##### 2.2 安装k8s软件

```
yum list --showduplicates|egrep kubeadm  ##查看哪些版本可用
yum install kubeadm-1.18.0-0 kubelet-1.18.0-0 kubectl-1.18.0-0 -y
##修改kubelet使用的pause镜像为阿里云的镜像，否则会从gcr.io仓库中拉取，大概率会失败
cat <<EOF >/etc/sysconfig/kubelet
KUBELET_EXTRA_ARGS="--cgroup-driver=systemd --pod-infra-container-image=registry.cn-hangzhou.aliyuncs.com/google_containers/pause-amd64:3.1"
EOF

systemctl daemon-reload && systemctl enable --now kubelet
```

##### 2.3 安装负载均衡及高可用服务

Kubernetes master 节点运行如下组件：

```
- kube-apiserver
- kube-scheduler
- kube-controller-manager
```

kube-scheduler 和 kube-controller-manager 可以以集群模式运行，通过 leader 选举产生一个工作进程，其它进程处于阻塞模式。**kube-apiserver可以运行多个实例，但对其它组件需要提供统一的访问地址，该地址需要高可用。**

kube-apiserver的端口为6443, 为避免冲突, haproxy 监听的端口要与之不同，此实验中为16443

```
yum install keepalived haproxy -y
##编辑haproxy配置文件
vi /etc/haproxy/haproxy.cfg
maxconn     51200  ##global配置中haproxy进程最大连接数的默认是4000。有点小可以调大
frontend k8s-master
  bind 0.0.0.0:16443
  bind 127.0.0.1:16443
  mode tcp
  option tcplog
  tcp-request inspect-delay 5s
  default_backend k8s-master

backend k8s-master
  mode tcp
  option tcplog
  option tcp-check
  balance roundrobin
  default-server inter 10s downinter 5s rise 2 fall 2 slowstart 60s maxconn 250 maxqueue 256 weight 100
  server master1        192.168.1.25:6443  check
  server master2        192.168.1.136:6443  check
  server master3        192.168.1.37:6443  check
  
  
##编辑keepalived配置文件
mv /etc/keepalived/keepalived.conf /etc/keepalived/keepalived.conf.bak
vi /etc/keepalived/keepalived.conf
! Configuration File for keepalived
global_defs {
    router_id LVS_DEVEL
}
vrrp_script chk_apiserver {
    script "/etc/keepalived/check_apiserver.sh"
    interval 2
    weight -5
    fall 3
    rise 2
}
vrrp_instance VI_1 {
    state MASTER		##slave节点为BACKUP
    interface eth0
    mcast_src_ip 192.168.1.25  ##各个master的IP
    virtual_router_id 61
    priority 100		##slave的数值要小，数值越大优先级越高
    advert_int 2
    authentication {
        auth_type PASS
        auth_pass K8SHA_KA_AUTH
    }
    virtual_ipaddress {
        192.168.1.228
    }
#    track_script {
#       chk_apiserver
#    }
}
##注意上述的健康检查是关闭的，集群建立完成后再开启


##配置健康检查脚本
cat << EOF > /etc/keepalived/check_apiserver.sh
#!/bin/bash
 
err=0
for k in $(seq 1 5)
do
    check_code=$(pgrep kube-apiserver)
    if [[ $check_code == "" ]]; then
        err=$(expr $err + 1)
        sleep 5
        continue
    else
        err=0
        break
    fi
done
 
if [[ $err != "0" ]]; then
    echo "systemctl stop keepalived"
    /usr/bin/systemctl stop keepalived
    exit 1
else
    exit 0
fi
EOF


##启动haproxy和keepalived
systemctl enable --now haproxy
systemctl enable --now keepalived
```



##### 2.4 初始化master1

在 Master1上创建初始化配置文件

```
mkdir k8s && cd k8s/ && kubeadm config print init-defaults > init.yml
```

根据实际环境修改初始化配置文件

```
apiVersion: kubeadm.k8s.io/v1beta2
bootstrapTokens:
- groups:
  - system:bootstrappers:kubeadm:default-node-token
    token: abcdef.0123456789abcdef
    ttl: 24h0m0s
    usages:
  - signing
  - authentication
    kind: InitConfiguration
    localAPIEndpoint:
    advertiseAddress: 192.168.1.25  ##此处更改为master1的IP
    bindPort: 6443
    nodeRegistration:
    criSocket: /var/run/dockershim.sock
    name: master1
    taints:
  - effect: NoSchedule
    key: node-role.kubernetes.io/master
---
apiServer:
  timeoutForControlPlane: 4m0s
apiVersion: kubeadm.k8s.io/v1beta2
certificatesDir: /etc/kubernetes/pki
clusterName: kubernetes
controlPlaneEndpoint: "192.168.1.228:16443"   # VIP：PORT
controllerManager: {}
dns:
  type: CoreDNS
etcd:
  local:
    dataDir: /var/lib/etcd
imageRepository: registry.aliyuncs.com/google_containers
kind: ClusterConfiguration
kubernetesVersion: v1.18.0
networking:
  dnsDomain: cluster.local
  serviceSubnet: 10.96.0.0/12
  podSubnet: 10.244.0.0/16    # pod子网，和Flannel中要一致
scheduler: {}
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
mode: ipvs
```

初始化master1

```
##提前下载镜像
kubeadm config images pull --config  init.yml -
kubeadm init --config=init.yml --upload-certs |tee kubeadm-init.log

W0519 08:51:22.123187    9005 configset.go:202] WARNING: kubeadm cannot validate component configs for API groups [kubelet.config.k8s.io kubeproxy.config.k8s.io]
[init] Using Kubernetes version: v1.18.0
[preflight] Running pre-flight checks
        [WARNING SystemVerification]: this Docker version is not on the list of validated versions: 20.10.6. Latest validated version: 19.03
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Starting the kubelet
[certs] Using certificateDir folder "/etc/kubernetes/pki"
[certs] Generating "ca" certificate and key
[certs] Generating "apiserver" certificate and key
[certs] apiserver serving cert is signed for DNS names [master1 kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 192.168.1.25 192.168.1.228]
[certs] Generating "apiserver-kubelet-client" certificate and key
[certs] Generating "front-proxy-ca" certificate and key
[certs] Generating "front-proxy-client" certificate and key
[certs] Generating "etcd/ca" certificate and key
[certs] Generating "etcd/server" certificate and key
[certs] etcd/server serving cert is signed for DNS names [master1 localhost] and IPs [192.168.1.25 127.0.0.1 ::1]
[certs] Generating "etcd/peer" certificate and key
[certs] etcd/peer serving cert is signed for DNS names [master1 localhost] and IPs [192.168.1.25 127.0.0.1 ::1]
[certs] Generating "etcd/healthcheck-client" certificate and key
[certs] Generating "apiserver-etcd-client" certificate and key
[certs] Generating "sa" key and public key
[kubeconfig] Using kubeconfig folder "/etc/kubernetes"
[endpoint] WARNING: port specified in controlPlaneEndpoint overrides bindPort in the controlplane address
[kubeconfig] Writing "admin.conf" kubeconfig file
[endpoint] WARNING: port specified in controlPlaneEndpoint overrides bindPort in the controlplane address
[kubeconfig] Writing "kubelet.conf" kubeconfig file
[endpoint] WARNING: port specified in controlPlaneEndpoint overrides bindPort in the controlplane address
[kubeconfig] Writing "controller-manager.conf" kubeconfig file
[endpoint] WARNING: port specified in controlPlaneEndpoint overrides bindPort in the controlplane address
[kubeconfig] Writing "scheduler.conf" kubeconfig file
[control-plane] Using manifest folder "/etc/kubernetes/manifests"
[control-plane] Creating static Pod manifest for "kube-apiserver"
W0519 08:51:29.621332    9005 manifests.go:225] the default kube-apiserver authorization-mode is "Node,RBAC"; using "Node,RBAC"
[control-plane] Creating static Pod manifest for "kube-controller-manager"
[control-plane] Creating static Pod manifest for "kube-scheduler"
W0519 08:51:29.622878    9005 manifests.go:225] the default kube-apiserver authorization-mode is "Node,RBAC"; using "Node,RBAC"
[etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests". This can take up to 4m0s
[apiclient] All control plane components are healthy after 25.625212 seconds
[upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
[kubelet] Creating a ConfigMap "kubelet-config-1.18" in namespace kube-system with the configuration for the kubelets in the cluster
[upload-certs] Storing the certificates in Secret "kubeadm-certs" in the "kube-system" Namespace
[upload-certs] Using certificate key:
367a3d53a6fcac096b8b9bde53b18a67a65e8862eb0f47090cea5df4c603fe68
[mark-control-plane] Marking the node master1 as control-plane by adding the label "node-role.kubernetes.io/master=''"
[mark-control-plane] Marking the node master1 as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule]
[bootstrap-token] Using token: abcdef.0123456789abcdef
[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to get nodes
[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
[bootstrap-token] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
[bootstrap-token] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
[bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
[kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
[addons] Applied essential addon: CoreDNS
[endpoint] WARNING: port specified in controlPlaneEndpoint overrides bindPort in the controlplane address
[addons] Applied essential addon: kube-proxy

Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

You can now join any number of the control-plane node running the following command on each as root:

  kubeadm join 192.168.1.228:16443 --token abcdef.0123456789abcdef \
    --discovery-token-ca-cert-hash sha256:0a2695f3b65a95f75712f75b70d0d2fa8918030af70c223e535e0ec65b80f21c \
    --control-plane --certificate-key 367a3d53a6fcac096b8b9bde53b18a67a65e8862eb0f47090cea5df4c603fe68

Please note that the certificate-key gives access to cluster sensitive data, keep it secret!
As a safeguard, uploaded-certs will be deleted in two hours; If necessary, you can use
"kubeadm init phase upload-certs --upload-certs" to reload certs afterward.

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 192.168.1.228:16443 --token abcdef.0123456789abcdef \
    --discovery-token-ca-cert-hash sha256:0a2695f3b65a95f75712f75b70d0d2fa8918030af70c223e535e0ec65b80f21c

```

kubeadm init主要执行了以下操作：
[init]：指定版本进行初始化操作
[preflight] ：初始化前的检查和下载所需要的Docker镜像文件
[kubelet-start]：生成kubelet的配置文件””/var/lib/kubelet/config.yaml”，没有这个文件kubelet无法启动，所以初始化之前的kubelet实际上启动不会成功。
[certificates]：生成Kubernetes使用的证书，存放在/etc/kubernetes/pki目录中。 [kubeconfig] ：生成 KubeConfig 文件，存放在/etc/kubernetes目录中，组件之间通信需要使用对应文件。
[control-plane]：使用/etc/kubernetes/manifest目录下的YAML文件，安装 Master 组件。
[etcd]：使用/etc/kubernetes/manifest/etcd.yaml安装Etcd服务。
[wait-control-plane]：等待control-plan部署的Master组件启动。
[apiclient]：检查Master组件服务状态。
[uploadconfig]：更新配置
[kubelet]：使用configMap配置kubelet。
[patchnode]：更新CNI信息到Node上，通过注释的方式记录。
[mark-control-plane]：为当前节点打标签，打了角色Master，和不可调度标签，这样默认就不会使用Master节点来运行Pod。
[bootstrap-token]：生成token记录下来，后边使用kubeadm join往集群中添加节点时会用到
[addons]：安装附加组件CoreDNS和kube-proxy

##### 2.5 配置kubectl 

无论在master节点或node节点，要能够执行kubectl命令必须进行配置，有两种配置方式

方式一：通过配置文件

```
mkdir -p $HOME/.kube
cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
chown $(id -u):$(id -g) $HOME/.kube/config
```

方式二：通过环境变量

```
echo 'export KUBECONFIG=/etc/kubernetes/admin.conf' >> ~/.bashrc
source ~/.bashrc
```

配置好kubectl后，就可以使用kubectl命令了

```
[root@mater1 k8s]# kubectl get cs     ##查看组件的状态
NAME                 STATUS    MESSAGE             ERROR
controller-manager   Healthy   ok
scheduler            Healthy   ok
etcd-0               Healthy   {"health":"true"}

[root@mater1 k8s]# kubectl get no     ##查看节点状态
NAME      STATUS     ROLES    AGE   VERSION
master1   NotReady   master   15m   v1.18.0                             ##由于还未安装网络插件，node处于NotReady状态
[root@mater1 k8s]# kubectl get po -n kube-system    ##查看kube-system命名空间下的pod状态
NAME                              READY   STATUS    RESTARTS   AGE
coredns-7ff77c879f-vk49q          0/1     Pending   0          15m       ##由于还没有部署网络插件，coredns处于pending状态
coredns-7ff77c879f-whc2f          0/1     Pending   0          15m
etcd-master1                      1/1     Running   0          15m
kube-apiserver-master1            1/1     Running   0          15m
kube-controller-manager-master1   1/1     Running   0          15m
kube-proxy-tsr8z                  1/1     Running   0          15m
kube-scheduler-master1            1/1     Running   0          15m

```

```
在更高版本的k8s集群中，可能会出现 scheduler、controller-manager 两个组件健康状况不正常的情况，如下：
[root@master-1 k8s]# kubectl get cs
NAME                 STATUS      MESSAGE                                                                                     ERROR
controller-manager   Unhealthy   Get http://127.0.0.1:10252/healthz: dial tcp 127.0.0.1:10252: connect: connection refused
scheduler            Unhealthy   Get http://127.0.0.1:10251/healthz: dial tcp 127.0.0.1:10251: connect: connection refused
etcd-0               Healthy     {"health":"true"}

解决方法：修改以下两个文件：
/etc/kubernetes/manifests/kube-controller-manager.yaml
/etc/kubernetes/manifests/kube-scheduler.yaml

注释掉： - --port=0  ，等一会即可
```

##### 2.6 部署网络插件

这里部署flannel插件

下载kube-flannel.yml文件

```
curl -O https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
```

由于kube-flannel.yml文件指定的镜像从coreos镜像仓库拉取，可能拉取失败，我们替换掉它

```
[root@mater1 k8s]# grep -i "flannel:" kube-flannel.yml
        image: quay.io/coreos/flannel:v0.14.0
        image: quay.io/coreos/flannel:v0.14.0

##提前下载好flannel镜像
docker pull cce-registry.ctyun.cn/caas/coreos/flannel:v0.10.0-amd64

#替换镜像仓库
sed -i 's#quay.io/coreos/flannel:v0.14.0#cce-registry.ctyun.cn/caas/coreos/flannel:v0.10.0-amd64#' kube-flannel.yml
```

执行安装

```
[root@mater1 k8s]# kubectl apply -f kube-flannel.yml
podsecuritypolicy.policy/psp.flannel.unprivileged created
clusterrole.rbac.authorization.k8s.io/flannel created
clusterrolebinding.rbac.authorization.k8s.io/flannel created
serviceaccount/flannel created
configmap/kube-flannel-cfg created
daemonset.apps/kube-flannel-ds created
```

再次查看node和 Pod状态，全部OK, 所有的核心组件都起来了

```
[root@mater1 k8s]# kubectl get po -n kube-system
NAME                              READY   STATUS    RESTARTS   AGE
coredns-7ff77c879f-vk49q          1/1     Running   0          5h16m
coredns-7ff77c879f-whc2f          1/1     Running   0          5h16m
etcd-master1                      1/1     Running   0          5h16m
kube-apiserver-master1            1/1     Running   0          5h16m
kube-controller-manager-master1   1/1     Running   0          5h16m
kube-flannel-ds-f26v4             1/1     Running   0          85s
kube-proxy-tsr8z                  1/1     Running   0          5h16m
kube-scheduler-master1            1/1     Running   0          5h16m
[root@mater1 k8s]#
[root@mater1 k8s]#
[root@mater1 k8s]#
[root@mater1 k8s]# kubectl get nodes
NAME      STATUS   ROLES    AGE     VERSION
master1   Ready    master   5h16m   v1.18.0
```



##### 2.7  加入master节点

```
  kubeadm join 192.168.1.228:16443 --token abcdef.0123456789abcdef \
    --discovery-token-ca-cert-hash sha256:0a2695f3b65a95f75712f75b70d0d2fa8918030af70c223e535e0ec65b80f21c \
    --control-plane --certificate-key 367a3d53a6fcac096b8b9bde53b18a67a65e8862eb0f47090cea5df4c603fe68

error execution phase control-plane-prepare/download-certs: error downloading certs: error downloading the secret: Secret "kubeadm-certs" was not found in the "kube-system" Namespace. This Secret might have expired. Please, run `kubeadm init phase upload-certs --upload-certs` on a control plane to generate a new one
To see the stack trace of this error execute with --v=5 or higher

```

#报错，显示secret无效了，在master1上重新生成下

```
##Master需要生成--certificate-key
##上传ca证书
[root@mater1 ~]# kubeadm init phase upload-certs
I0519 15:26:25.816658   22385 version.go:252] remote version is much newer: v1.21.1; falling back to: stable-1.18
W0519 15:26:27.595865   22385 configset.go:202] WARNING: kubeadm cannot validate component configs for API groups [kubelet.config.k8s.io kubeproxy.config.k8s.io]
[upload-certs] Skipping phase. Please see --upload-certs

##这里显示忽略上传，那么就要手动拷贝证书文件了[总共8个文件]
##在要加入的master节点上执行如下操作
mkdir -p /etc/kubernetes/pki
mkdir -p /etc/kubernetes/pki/etcd
scp master1:/etc/kubernetes/pki/ca.* /etc/kubernetes/pki/
scp master1:/etc/kubernetes/pki/sa.* /etc/kubernetes/pki/
scp master1:/etc/kubernetes/pki/front-proxy-ca*  /etc/kubernetes/pki/
scp master1:/etc/kubernetes/pki/etcd/ca*  /etc/kubernetes/pki/etcd/

[root@mater1 ~]# kubeadm token create --print-join-command
W0519 15:26:49.622838   22597 configset.go:202] WARNING: kubeadm cannot validate component configs for API groups [kubelet.config.k8s.io kubeproxy.config.k8s.io]
kubeadm join 192.168.1.228:16443 --token c4xy67.yw0me9xmnpqy3amg     --discovery-token-ca-cert-hash sha256:0a2695f3b65a95f75712f75b70d0d2fa8918030af70c223e535e0ec65b80f21c

#在上述命令生成的结果后面加上--control-plane
[root@master2 ~]# kubeadm join 192.168.1.228:16443 --token 7xe7sq.jvrwekuw3130bmgm     --discovery-token-ca-cert-hash sha256:0a2695f3b65a95f75712f75b70d0d2fa8918030af70c223e535e0ec65b80f21c --control-plane
[preflight] Running pre-flight checks
        [WARNING SystemVerification]: this Docker version is not on the list of validated versions: 20.10.6. Latest validated version: 19.03
[preflight] Reading configuration from the cluster...
[preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -oyaml'
[preflight] Running pre-flight checks before initializing the new control plane instance
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
[certs] Using certificateDir folder "/etc/kubernetes/pki"
[certs] Generating "etcd/server" certificate and key
[certs] etcd/server serving cert is signed for DNS names [master2 localhost] and IPs [192.168.1.136 127.0.0.1 ::1]
[certs] Generating "etcd/peer" certificate and key
[certs] etcd/peer serving cert is signed for DNS names [master2 localhost] and IPs [192.168.1.136 127.0.0.1 ::1]
[certs] Generating "apiserver-etcd-client" certificate and key
[certs] Generating "etcd/healthcheck-client" certificate and key
[certs] Generating "apiserver" certificate and key
[certs] apiserver serving cert is signed for DNS names [master2 kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 192.168.1.136 192.168.1.228]
[certs] Generating "apiserver-kubelet-client" certificate and key
[certs] Generating "front-proxy-client" certificate and key
[certs] Valid certificates and keys now exist in "/etc/kubernetes/pki"
[certs] Using the existing "sa" key
[kubeconfig] Generating kubeconfig files
[kubeconfig] Using kubeconfig folder "/etc/kubernetes"
[endpoint] WARNING: port specified in controlPlaneEndpoint overrides bindPort in the controlplane address
[kubeconfig] Writing "admin.conf" kubeconfig file
[kubeconfig] Writing "controller-manager.conf" kubeconfig file
[kubeconfig] Writing "scheduler.conf" kubeconfig file
[control-plane] Using manifest folder "/etc/kubernetes/manifests"
[control-plane] Creating static Pod manifest for "kube-apiserver"
W0519 15:42:29.561394   25747 manifests.go:225] the default kube-apiserver authorization-mode is "Node,RBAC"; using "Node,RBAC"
[control-plane] Creating static Pod manifest for "kube-controller-manager"
W0519 15:42:29.572406   25747 manifests.go:225] the default kube-apiserver authorization-mode is "Node,RBAC"; using "Node,RBAC"
[control-plane] Creating static Pod manifest for "kube-scheduler"
W0519 15:42:29.573716   25747 manifests.go:225] the default kube-apiserver authorization-mode is "Node,RBAC"; using "Node,RBAC"
[check-etcd] Checking that the etcd cluster is healthy
[kubelet-start] Downloading configuration for the kubelet from the "kubelet-config-1.18" ConfigMap in the kube-system namespace
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Starting the kubelet
[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...
[etcd] Announced new etcd member joining to the existing etcd cluster
[etcd] Creating static Pod manifest for "etcd"
[etcd] Waiting for the new etcd member to join the cluster. This can take up to 40s
{"level":"warn","ts":"2021-05-19T15:42:51.479+0800","caller":"clientv3/retry_interceptor.go:61","msg":"retrying of unary invoker failed","target":"passthrough:///https://192.168.1.136:2379","attempt":0,"error":"rpc error: code = DeadlineExceeded desc = context deadline exceeded"}
[upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
[mark-control-plane] Marking the node master2 as control-plane by adding the label "node-role.kubernetes.io/master=''"
[mark-control-plane] Marking the node master2 as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule]

This node has joined the cluster and a new control plane instance was created:

* Certificate signing request was sent to apiserver and approval was received.
* The Kubelet was informed of the new secure connection details.
* Control plane (master) label and taint were applied to the new node.
* The Kubernetes control plane instances scaled up.
* A new etcd member was added to the local/stacked etcd cluster.

To start administering your cluster from this node, you need to run the following as a regular user:

        mkdir -p $HOME/.kube
        sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
        sudo chown $(id -u):$(id -g) $HOME/.kube/config

Run 'kubectl get nodes' to see this node join the cluster.

```

配置kubelet

```
[root@master2 ~]# mkdir -p $HOME/.kube && cp -i /etc/kubernetes/admin.conf $HOME/.kube/config && chown $(id -u):$(id -g) $HOME/.kube/config
```

```
[root@master2 ~]# kubectl get nodes
NAME      STATUS   ROLES    AGE     VERSION
master1   Ready    master   7h35m   v1.18.0
master2   Ready    master   45m     v1.18.0
```

##master3节点也执行上述命令加入集群

##### 2.8 加入node节点

```
kubeadm join 192.168.1.228:16443 --token 7xe7sq.jvrwekuw3130bmgm     --discovery-token-ca-cert-hash sha256:0a2695f3b65a95f75712f75b70d0d2fa8918030af70c223e535e0ec65b80f21c
W0519 16:45:21.361896    6294 join.go:346] [preflight] WARNING: JoinControlPane.controlPlane settings will be ignored when control-plane flag is not set.
[preflight] Running pre-flight checks
        [WARNING SystemVerification]: this Docker version is not on the list of validated versions: 20.10.6. Latest validated version: 19.03
[preflight] Reading configuration from the cluster...
[preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -oyaml'
[kubelet-start] Downloading configuration for the kubelet from the "kubelet-config-1.18" ConfigMap in the kube-system namespace
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Starting the kubelet
[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...

This node has joined the cluster:
* Certificate signing request was sent to apiserver and a response was received.
* The Kubelet was informed of the new secure connection details.

Run 'kubectl get nodes' on the control-plane to see this node join the cluster.


```

```
[root@mater1 ~]# kubectl get no
NAME      STATUS   ROLES    AGE     VERSION
master1   Ready    master   7h54m   v1.18.0
master2   Ready    master   63m     v1.18.0
master3   Ready    master   14m     v1.18.0
node1     Ready    <none>   9m      v1.18.0
node2     Ready    <none>   4m13s   v1.18.0
node3     Ready    <none>   53s     v1.18.0
```



##### 2.9 更改keepalived配置，取消掉注释

    track_script {
       chk_apiserver
    }
```
systemctl restart keepalived
```



##### 2.10 Metrics部署

在新版的Kubernetes中系统资源的采集均使用Metrics-server，可以通过Metrics采集节点和Pod的内存、磁盘、CPU和网络的使用率

<!--注：heapster模块在1.8版本之后由metricserver代替-->

[k8s安装metrics-server]: https://www.cnblogs.com/lfl17718347843/p/14283796.html

##检查API server是否开启了Aggregator Routing：查看API Server是否具有--enable-aggregator-routing=true

```
[root@mater1 metrics]# ps -ef |grep apiserver
root     10073 10053  5 May19 ?        01:17:14 kube-apiserver --advertise-address=192.168.1.25 --allow-privileged=true --authorization-mode=Node,RBAC --client-ca-file=/etc/kubernetes/pki/ca.crt --enable-admission-plugins=NodeRestriction --enable-bootstrap-token-auth=true --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key --etcd-servers=https://127.0.0.1:2379 --insecure-port=0 --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key --requestheader-allowed-names=front-proxy-client --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt --requestheader-extra-headers-prefix=X-Remote-Extra- --requestheader-group-headers=X-Remote-Group --requestheader-username-headers=X-Remote-User --secure-port=6443 --service-account-key-file=/etc/kubernetes/pki/sa.pub --service-cluster-ip-range=10.96.0.0/12 --tls-cert-file=/etc/kubernetes/pki/apiserver.crt --tls-private-key-file=/etc/kubernetes/pki/apiserver.key
root     11187 22207  0 08:56 pts/0    00:00:00 grep --color=auto apiserver
```

##修改每个 API Server 的 kube-apiserver.yaml 配置开启 Aggregator Routing：修改 manifests 配置后 API Server 会自动重启生效。

```
vi /etc/kubernetes/manifests/kube-apiserver.yaml
- --enable-aggregator-routing=true  ##添加此行
```

```
[root@mater1 metrics]# ps -ef |grep apiserver  |grep rout
root     12507 12486 31 08:59 ?        00:00:06 kube-apiserver --advertise-address=192.168.1.25 --allow-privileged=true --authorization-mode=Node,RBAC --client-ca-file=/etc/kubernetes/pki/ca.crt --enable-admission-plugins=NodeRestriction --enable-bootstrap-token-auth=true --enable-aggregator-routing=true --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key --etcd-servers=https://127.0.0.1:2379 --insecure-port=0 --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key --requestheader-allowed-names=front-proxy-client --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt --requestheader-extra-headers-prefix=X-Remote-Extra- --requestheader-group-headers=X-Remote-Group --requestheader-username-headers=X-Remote-User --secure-port=6443 --service-account-key-file=/etc/kubernetes/pki/sa.pub --service-cluster-ip-range=10.96.0.0/12 --tls-cert-file=/etc/kubernetes/pki/apiserver.crt --tls-private-key-file=/etc/kubernetes/pki/apiserver.key
[root@mater1 metrics]#
```

可以看到已经生效了

##yaml文件

  - ```
    [root@mater1 metrics]# cat metrics-server.yaml
    ---
    
    apiVersion: rbac.authorization.k8s.io/v1
    kind: ClusterRoleBinding
    metadata:
      name: metrics-server:system:auth-delegator
      labels:
        kubernetes.io/cluster-service: "true"
        addonmanager.kubernetes.io/mode: Reconcile
    roleRef:
      apiGroup: rbac.authorization.k8s.io
      kind: ClusterRole
      name: system:auth-delegator
    subjects:
    
    - kind: ServiceAccount
      name: metrics-server
      namespace: kube-system
    
    ---
    
    apiVersion: rbac.authorization.k8s.io/v1
    kind: RoleBinding
    metadata:
      name: metrics-server-auth-reader
      namespace: kube-system
      labels:
        kubernetes.io/cluster-service: "true"
        addonmanager.kubernetes.io/mode: Reconcile
    roleRef:
      apiGroup: rbac.authorization.k8s.io
      kind: Role
      name: extension-apiserver-authentication-reader
    subjects:
    
    - kind: ServiceAccount
      name: metrics-server
      namespace: kube-system
    
    ---
    
    apiVersion: rbac.authorization.k8s.io/v1
    kind: ClusterRole
    metadata:
      name: system:metrics-server
      labels:
        kubernetes.io/cluster-service: "true"
        addonmanager.kubernetes.io/mode: Reconcile
    rules:
    
    - apiGroups:
      - ""
        resources:
      - pods
      - nodes
      - nodes/stats
      - namespaces
        verbs:
      - get
      - list
      - watch
    - apiGroups:
      - "extensions"
        resources:
      - deployments
        verbs:
      - get
      - list
      - update
      - watch
    
    ---
    
    apiVersion: rbac.authorization.k8s.io/v1
    kind: ClusterRoleBinding
    metadata:
      name: system:metrics-server
      labels:
        kubernetes.io/cluster-service: "true"
        addonmanager.kubernetes.io/mode: Reconcile
    roleRef:
      apiGroup: rbac.authorization.k8s.io
      kind: ClusterRole
      name: system:metrics-server
    subjects:
    
    - kind: ServiceAccount
      name: metrics-server
      namespace: kube-system
    
    ---
    
    apiVersion: apiregistration.k8s.io/v1beta1
    kind: APIService
    metadata:
      name: v1beta1.metrics.k8s.io
      labels:
        kubernetes.io/cluster-service: "true"
        addonmanager.kubernetes.io/mode: Reconcile
    spec:
      service:
        name: metrics-server
        namespace: kube-system
      group: metrics.k8s.io
      version: v1beta1
      insecureSkipTLSVerify: true
      groupPriorityMinimum: 100
    
      versionPriority: 100
    ---
    
    apiVersion: v1
    kind: ServiceAccount
    metadata:
      name: metrics-server
      namespace: kube-system
      labels:
        kubernetes.io/cluster-service: "true"
    
        addonmanager.kubernetes.io/mode: Reconcile
    ---
    
    apiVersion: v1
    kind: ConfigMap
    metadata:
      name: metrics-server-config
      namespace: kube-system
      labels:
        kubernetes.io/cluster-service: "true"
        addonmanager.kubernetes.io/mode: EnsureExists
    data:
      NannyConfiguration: |-
        apiVersion: nannyconfig/v1alpha1
    
        kind: NannyConfiguration
    ---
    
    apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: metrics-server-v0.3.2
      namespace: kube-system
      labels:
        k8s-app: metrics-server
        kubernetes.io/cluster-service: "true"
        addonmanager.kubernetes.io/mode: Reconcile
        version: v0.3.2
    spec:
      selector:
        matchLabels:
          k8s-app: metrics-server
          version: v0.3.2
      template:
        metadata:
          name: metrics-server
          labels:
            k8s-app: metrics-server
            version: v0.3.2
          annotations:
            scheduler.alpha.kubernetes.io/critical-pod: ''
            seccomp.security.alpha.kubernetes.io/pod: 'docker/default'
        spec:
          priorityClassName: system-cluster-critical
          serviceAccountName: metrics-server
          containers:
          - name: metrics-server
            image: "cce-registry.ctyun.cn/caas/metrics-server-amd64:v0.3.3"
            command:
            - /metrics-server
            - --metric-resolution=30s
            - --kubelet-insecure-tls
            - --kubelet-preferred-address-types=InternalIP,Hostname,InternalDNS,ExternalDNS,ExternalIP
            # These are needed for GKE, which doesn't support secure communication yet.
            # Remove these lines for non-GKE clusters, and when GKE supports token-based auth.
            #- --kubelet-port=10255
            #- --deprecated-kubelet-completely-insecure=true
            #- --kubelet-preferred-address-types=InternalIP,Hostname,InternalDNS,ExternalDNS,ExternalIP
            ports:
            - containerPort: 443
              name: https
              protocol: TCP
          - name: metrics-server-nanny
            image: "cce-registry.ctyun.cn/caas/addon-resizer:1.8.4"
            resources:
              limits:
                cpu: 100m
                memory: 300Mi
              requests:
                cpu: 5m
                memory: 50Mi
            env:
              - name: MY_POD_NAME
                valueFrom:
                  fieldRef:
                    fieldPath: metadata.name
              - name: MY_POD_NAMESPACE
                valueFrom:
                  fieldRef:
                    fieldPath: metadata.namespace
            volumeMounts:
            - name: metrics-server-config-volume
              mountPath: /etc/config
            command:
              - /pod_nanny
              - --config-dir=/etc/config
              - --cpu=80m
              - --extra-cpu=0.5m
              - --memory=80Mi
              - --extra-memory=8Mi
              - --threshold=5
              - --deployment=metrics-server-v0.3.2
              - --container=metrics-server
              - --poll-period=300000
              - --estimator=exponential
              # Specifies the smallest cluster (defined in number of nodes)
              # resources will be scaled to.
              #- --minClusterSize=3
          volumes:
            - name: metrics-server-config-volume
              configMap:
                name: metrics-server-config
          tolerations:
            - key: "CriticalAddonsOnly"
              operator: "Exists"
            - effect: NoSchedule
              key: node-role.kubernetes.io/master
            - effect: NoExecute
              key: node.kubernetes.io/not-ready
              operator: Exists
              tolerationSeconds: 300
            - effect: NoExecute
              key: node.kubernetes.io/unreachable
              operator: Exists
              tolerationSeconds: 300
          nodeSelector:
    
            node-role.kubernetes.io/master: ""
    ---
    
    apiVersion: v1
    kind: Service
    metadata:
      name: metrics-server
      namespace: kube-system
      labels:
        addonmanager.kubernetes.io/mode: Reconcile
        kubernetes.io/cluster-service: "true"
        kubernetes.io/name: "Metrics-server"
    spec:
      selector:
        k8s-app: metrics-server
      ports:
    
      - port: 443
        protocol: TCP
        targetPort: https
    ```

##安装

```
kubectl apply -f metrics-server.yaml
```

##确认pod deployment 和服务的运行状态

```
[root@mater1 metrics]# kubectl get pod -A |grep metrics
kube-system   metrics-server-v0.3.2-7d769d6cd6-g7x48   2/2     Running   0          4m11s
[root@mater1 metrics]# kubectl get deployment -A |grep metrics
kube-system   metrics-server-v0.3.2   1/1     1            1           4m17s
[root@mater1 metrics]# kubectl get service -A |grep metrics
kube-system   metrics-server   ClusterIP   10.105.87.80   <none>        443/TCP                  4m24s
```

##使用kubectl top 确认pod node资源信息

```
[root@mater1 metrics]# kubectl top pod metrics-server-v0.3.2-7d769d6cd6-g7x48 -n kube-system
NAME                                     CPU(cores)   MEMORY(bytes)
metrics-server-v0.3.2-7d769d6cd6-g7x48   2m           21Mi
[root@mater1 metrics]# kubectl top node
NAME      CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%
master1   153m         3%     2136Mi          27%
master2   184m         4%     1399Mi          18%
master3   154m         3%     1314Mi          17%
node1     49m          1%     550Mi           7%
node2     48m          1%     529Mi           6%
node3     49m          1%     533Mi           6%

```



##### 2.11 Dashboard部署

##dashboard和k8s的版本兼容

https://github.com/kubernetes/dashboard/releases

这里选择v2.0.3版本的

```
kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.3/aio/deploy/recommended.yaml
```

#查看dashboard运行状态，以deployment方式部署，运行2个pod及2个service：

```
[root@mater1 dashboard]# kubectl -n kubernetes-dashboard get pods
NAME                                         READY   STATUS    RESTARTS   AGE
dashboard-metrics-scraper-6b4884c9d5-5pzdc   1/1     Running   0          30m
kubernetes-dashboard-7f99b75bf4-n4p6r        1/1     Running   0          30m
[root@mater1 dashboard]# kubectl -n kubernetes-dashboard get svc
NAME                        TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE
dashboard-metrics-scraper   ClusterIP   10.106.44.197   <none>        8000/TCP   30m
kubernetes-dashboard        ClusterIP   10.99.198.108   <none>        443/TCP    30m
```

#这里作为演示，使用nodeport方式将dashboard服务暴露在集群外，指定使用30443端口，可自定义：

```
[root@mater1 dashboard]# kubectl  patch svc kubernetes-dashboard -n kubernetes-dashboard -p '{"spec":{"type":"NodePort","ports":[{"port":443,"targetPort":8443,"nodePort":30443}]}}'
service/kubernetes-dashboard patched

```

#查看暴露的service,已修改为nodeport类型：

```
[root@mater1 dashboard]# kubectl -n kubernetes-dashboard get svc
NAME                        TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)         AGE
dashboard-metrics-scraper   ClusterIP   10.106.44.197   <none>        8000/TCP        33m
kubernetes-dashboard        NodePort    10.99.198.108   <none>        443:30443/TCP   33m
```

##Dashboard 支持 Kubeconfig 和 Token 两种认证方式，我们这里选择Token认证方式登录

##创建dashboard-adminuser.yaml

- ```
  cat > dashboard-adminuser.yaml << EOF
  apiVersion: v1
kind: ServiceAccount
metadata:
  name: admin-user
  namespace: kubernetes-dashboard
  
  ---
  
  apiVersion: rbac.authorization.k8s.io/v1
  kind: ClusterRoleBinding
  metadata:
    name: admin-user
  roleRef:
    apiGroup: rbac.authorization.k8s.io
    kind: ClusterRole
    name: cluster-admin
  subjects:
  
  - kind: ServiceAccount
    name: admin-user
    namespace: kubernetes-dashboard  
  EOF
  ```
  
  ##创建登录用户
  
  ```
  [root@mater1 dashboard]# kubectl apply -f dashboard-adminuser.yaml
  serviceaccount/admin-user created
  clusterrolebinding.rbac.authorization.k8s.io/admin-user created
  ```
  
  <!--说明：上面创建了一个叫admin-user的服务账号，并放在kubernetes-dashboard 命名空间下，并将cluster-admin角色绑定到admin-user账户，这样admin-user账户就有了管理员的权限。默认情况下，kubeadm创建集群时已经创建了cluster-admin角色，我们直接绑定即可-->
  
  ##查看admin-user账户的token
  
  ```
  kubectl -n kubernetes-dashboard describe secret $(kubectl -n kubernetes-dashboard get secret | grep admin-user | awk '{print $1}')
  ```
  
  ##把获取到的Token复制到登录界面的Token输入框中，成功登陆dashboard:
  
  ```
  https://<ip>:30443
  ```
  
  ##界面展示
  
  ![](https://i.loli.net/2021/05/20/jgZxMbaScl83iDN.png)

##可以看到还是有点权限的问题，不能展示具体的项目